<!doctype html>
<head>
  <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />

    <title>latex_ocr.model.components.attention_cell API documentation</title>
    <meta name="description" content="" />

  <link href='http://fonts.googleapis.com/css?family=Source+Sans+Pro:400,300' rel='stylesheet' type='text/css'>
  
  <style type="text/css">
  
* {
  box-sizing: border-box;
}
/*! normalize.css v1.1.1 | MIT License | git.io/normalize */

/* ==========================================================================
   HTML5 display definitions
   ========================================================================== */

/**
 * Correct `block` display not defined in IE 6/7/8/9 and Firefox 3.
 */

article,
aside,
details,
figcaption,
figure,
footer,
header,
hgroup,
main,
nav,
section,
summary {
    display: block;
}

/**
 * Correct `inline-block` display not defined in IE 6/7/8/9 and Firefox 3.
 */

audio,
canvas,
video {
    display: inline-block;
    *display: inline;
    *zoom: 1;
}

/**
 * Prevent modern browsers from displaying `audio` without controls.
 * Remove excess height in iOS 5 devices.
 */

audio:not([controls]) {
    display: none;
    height: 0;
}

/**
 * Address styling not present in IE 7/8/9, Firefox 3, and Safari 4.
 * Known issue: no IE 6 support.
 */

[hidden] {
    display: none;
}

/* ==========================================================================
   Base
   ========================================================================== */

/**
 * 1. Prevent system color scheme's background color being used in Firefox, IE,
 *    and Opera.
 * 2. Prevent system color scheme's text color being used in Firefox, IE, and
 *    Opera.
 * 3. Correct text resizing oddly in IE 6/7 when body `font-size` is set using
 *    `em` units.
 * 4. Prevent iOS text size adjust after orientation change, without disabling
 *    user zoom.
 */

html {
    background: #fff; /* 1 */
    color: #000; /* 2 */
    font-size: 100%; /* 3 */
    -webkit-text-size-adjust: 100%; /* 4 */
    -ms-text-size-adjust: 100%; /* 4 */
}

/**
 * Address `font-family` inconsistency between `textarea` and other form
 * elements.
 */

html,
button,
input,
select,
textarea {
    font-family: sans-serif;
}

/**
 * Address margins handled incorrectly in IE 6/7.
 */

body {
    margin: 0;
}

/* ==========================================================================
   Links
   ========================================================================== */

/**
 * Address `outline` inconsistency between Chrome and other browsers.
 */

a:focus {
    outline: thin dotted;
}

/**
 * Improve readability when focused and also mouse hovered in all browsers.
 */

a:active,
a:hover {
    outline: 0;
}

/* ==========================================================================
   Typography
   ========================================================================== */

/**
 * Address font sizes and margins set differently in IE 6/7.
 * Address font sizes within `section` and `article` in Firefox 4+, Safari 5,
 * and Chrome.
 */

h1 {
    font-size: 2em;
    margin: 0.67em 0;
}

h2 {
    font-size: 1.5em;
    margin: 0.83em 0;
}

h3 {
    font-size: 1.17em;
    margin: 1em 0;
}

h4 {
    font-size: 1em;
    margin: 1.33em 0;
}

h5 {
    font-size: 0.83em;
    margin: 1.67em 0;
}

h6 {
    font-size: 0.67em;
    margin: 2.33em 0;
}

/**
 * Address styling not present in IE 7/8/9, Safari 5, and Chrome.
 */

abbr[title] {
    border-bottom: 1px dotted;
}

/**
 * Address style set to `bolder` in Firefox 3+, Safari 4/5, and Chrome.
 */

b,
strong {
    font-weight: bold;
}

blockquote {
    margin: 1em 40px;
}

/**
 * Address styling not present in Safari 5 and Chrome.
 */

dfn {
    font-style: italic;
}

/**
 * Address differences between Firefox and other browsers.
 * Known issue: no IE 6/7 normalization.
 */

hr {
    -moz-box-sizing: content-box;
    box-sizing: content-box;
    height: 0;
}

/**
 * Address styling not present in IE 6/7/8/9.
 */

mark {
    background: #ff0;
    color: #000;
}

/**
 * Address margins set differently in IE 6/7.
 */

p,
pre {
    margin: 1em 0;
}

/**
 * Correct font family set oddly in IE 6, Safari 4/5, and Chrome.
 */

code,
kbd,
pre,
samp {
    font-family: monospace, serif;
    _font-family: 'courier new', monospace;
    font-size: 1em;
}

/**
 * Improve readability of pre-formatted text in all browsers.
 */

pre {
    white-space: pre;
    white-space: pre-wrap;
    word-wrap: break-word;
}

/**
 * Address CSS quotes not supported in IE 6/7.
 */

q {
    quotes: none;
}

/**
 * Address `quotes` property not supported in Safari 4.
 */

q:before,
q:after {
    content: '';
    content: none;
}

/**
 * Address inconsistent and variable font size in all browsers.
 */

small {
    font-size: 80%;
}

/**
 * Prevent `sub` and `sup` affecting `line-height` in all browsers.
 */

sub,
sup {
    font-size: 75%;
    line-height: 0;
    position: relative;
    vertical-align: baseline;
}

sup {
    top: -0.5em;
}

sub {
    bottom: -0.25em;
}

/* ==========================================================================
   Lists
   ========================================================================== */

/**
 * Address margins set differently in IE 6/7.
 */

dl,
menu,
ol,
ul {
    margin: 1em 0;
}

dd {
    margin: 0 0 0 40px;
}

/**
 * Address paddings set differently in IE 6/7.
 */

menu,
ol,
ul {
    padding: 0 0 0 40px;
}

/**
 * Correct list images handled incorrectly in IE 7.
 */

nav ul,
nav ol {
    list-style: none;
    list-style-image: none;
}

/* ==========================================================================
   Embedded content
   ========================================================================== */

/**
 * 1. Remove border when inside `a` element in IE 6/7/8/9 and Firefox 3.
 * 2. Improve image quality when scaled in IE 7.
 */

img {
    border: 0; /* 1 */
    -ms-interpolation-mode: bicubic; /* 2 */
}

/**
 * Correct overflow displayed oddly in IE 9.
 */

svg:not(:root) {
    overflow: hidden;
}

/* ==========================================================================
   Figures
   ========================================================================== */

/**
 * Address margin not present in IE 6/7/8/9, Safari 5, and Opera 11.
 */

figure {
    margin: 0;
}

/* ==========================================================================
   Forms
   ========================================================================== */

/**
 * Correct margin displayed oddly in IE 6/7.
 */

form {
    margin: 0;
}

/**
 * Define consistent border, margin, and padding.
 */

fieldset {
    border: 1px solid #c0c0c0;
    margin: 0 2px;
    padding: 0.35em 0.625em 0.75em;
}

/**
 * 1. Correct color not being inherited in IE 6/7/8/9.
 * 2. Correct text not wrapping in Firefox 3.
 * 3. Correct alignment displayed oddly in IE 6/7.
 */

legend {
    border: 0; /* 1 */
    padding: 0;
    white-space: normal; /* 2 */
    *margin-left: -7px; /* 3 */
}

/**
 * 1. Correct font size not being inherited in all browsers.
 * 2. Address margins set differently in IE 6/7, Firefox 3+, Safari 5,
 *    and Chrome.
 * 3. Improve appearance and consistency in all browsers.
 */

button,
input,
select,
textarea {
    font-size: 100%; /* 1 */
    margin: 0; /* 2 */
    vertical-align: baseline; /* 3 */
    *vertical-align: middle; /* 3 */
}

/**
 * Address Firefox 3+ setting `line-height` on `input` using `!important` in
 * the UA stylesheet.
 */

button,
input {
    line-height: normal;
}

/**
 * Address inconsistent `text-transform` inheritance for `button` and `select`.
 * All other form control elements do not inherit `text-transform` values.
 * Correct `button` style inheritance in Chrome, Safari 5+, and IE 6+.
 * Correct `select` style inheritance in Firefox 4+ and Opera.
 */

button,
select {
    text-transform: none;
}

/**
 * 1. Avoid the WebKit bug in Android 4.0.* where (2) destroys native `audio`
 *    and `video` controls.
 * 2. Correct inability to style clickable `input` types in iOS.
 * 3. Improve usability and consistency of cursor style between image-type
 *    `input` and others.
 * 4. Remove inner spacing in IE 7 without affecting normal text inputs.
 *    Known issue: inner spacing remains in IE 6.
 */

button,
html input[type="button"], /* 1 */
input[type="reset"],
input[type="submit"] {
    -webkit-appearance: button; /* 2 */
    cursor: pointer; /* 3 */
    *overflow: visible;  /* 4 */
}

/**
 * Re-set default cursor for disabled elements.
 */

button[disabled],
html input[disabled] {
    cursor: default;
}

/**
 * 1. Address box sizing set to content-box in IE 8/9.
 * 2. Remove excess padding in IE 8/9.
 * 3. Remove excess padding in IE 7.
 *    Known issue: excess padding remains in IE 6.
 */

input[type="checkbox"],
input[type="radio"] {
    box-sizing: border-box; /* 1 */
    padding: 0; /* 2 */
    *height: 13px; /* 3 */
    *width: 13px; /* 3 */
}

/**
 * 1. Address `appearance` set to `searchfield` in Safari 5 and Chrome.
 * 2. Address `box-sizing` set to `border-box` in Safari 5 and Chrome
 *    (include `-moz` to future-proof).
 */

input[type="search"] {
    -webkit-appearance: textfield; /* 1 */
    -moz-box-sizing: content-box;
    -webkit-box-sizing: content-box; /* 2 */
    box-sizing: content-box;
}

/**
 * Remove inner padding and search cancel button in Safari 5 and Chrome
 * on OS X.
 */

input[type="search"]::-webkit-search-cancel-button,
input[type="search"]::-webkit-search-decoration {
    -webkit-appearance: none;
}

/**
 * Remove inner padding and border in Firefox 3+.
 */

button::-moz-focus-inner,
input::-moz-focus-inner {
    border: 0;
    padding: 0;
}

/**
 * 1. Remove default vertical scrollbar in IE 6/7/8/9.
 * 2. Improve readability and alignment in all browsers.
 */

textarea {
    overflow: auto; /* 1 */
    vertical-align: top; /* 2 */
}

/* ==========================================================================
   Tables
   ========================================================================== */

/**
 * Remove most spacing between table cells.
 */

table {
    border-collapse: collapse;
    border-spacing: 0;
}

  </style>

  <style type="text/css">
  
  html, body {
    margin: 0;
    padding: 0;
    min-height: 100%;
  }
  body {
    background: #fff;
    font-family: "Source Sans Pro", "Helvetica Neueue", Helvetica, sans;
    font-weight: 300;
    font-size: 16px;
    line-height: 1.6em;
  }
  #content {
    width: 70%;
    max-width: 850px;
    float: left;
    padding: 30px 60px;
    border-left: 1px solid #ddd;
  }
  #sidebar {
    width: 25%;
    float: left;
    padding: 30px;
    overflow: hidden;
  }
  #nav {
    font-size: 130%;
    margin: 0 0 15px 0;
  }

  #top {
    display: block;
    position: fixed;
    bottom: 5px;
    left: 5px;
    font-size: .85em;
    text-transform: uppercase;
  }

  #footer {
    font-size: .75em;
    padding: 5px 30px;
    border-top: 1px solid #ddd;
    text-align: right;
  }
    #footer p {
      margin: 0 0 0 30px;
      display: inline-block;
    }

  h1, h2, h3, h4, h5 {
    font-weight: 300;
  }
  h1 {
    font-size: 2.5em;
    line-height: 1.1em;
    margin: 0 0 .50em 0;
  }

  h2 {
    font-size: 1.75em;
    margin: 1em 0 .50em 0;
  }

  h3 {
    margin: 25px 0 10px 0;
  }

  h4 {
    margin: 0;
    font-size: 105%;
  }

  a {
    color: #058;
    text-decoration: none;
    transition: color .3s ease-in-out;
  }

  a:hover {
    color: #e08524;
    transition: color .3s ease-in-out;
  }

  pre, code, .mono, .name {
    font-family: "Ubuntu Mono", "Cousine", "DejaVu Sans Mono", monospace;
  }

  .title .name {
    font-weight: bold;
  }
  .section-title {
    margin-top: 2em;
  }
  .ident {
    color: #900;
  }

  code {
    background: #f9f9f9;
  } 

  pre {
    background: #fefefe;
    border: 1px solid #ddd;
    box-shadow: 2px 2px 0 #f3f3f3;
    margin: 0 30px;
    padding: 15px 30px;
  }

  .codehilite {
    margin: 0 30px 10px 30px;
  }

    .codehilite pre {
      margin: 0;
    }
    .codehilite .err { background: #ff3300; color: #fff !important; } 

  table#module-list {
    font-size: 110%;
  }

    table#module-list tr td:first-child {
      padding-right: 10px;
      white-space: nowrap;
    }

    table#module-list td {
      vertical-align: top;
      padding-bottom: 8px;
    }

      table#module-list td p {
        margin: 0 0 7px 0;
      }

  .def {
    display: table;
  }

    .def p {
      display: table-cell;
      vertical-align: top;
      text-align: left;
    }

    .def p:first-child {
      white-space: nowrap;
    }

    .def p:last-child {
      width: 100%;
    }


  #index {
    list-style-type: none;
    margin: 0;
    padding: 0;
  }
    ul#index .class_name {
      /* font-size: 110%; */
      font-weight: bold;
    }
    #index ul {
      margin: 0;
    }

  .item {
    margin: 0 0 15px 0;
  }

    .item .class {
      margin: 0 0 25px 30px;
    }

      .item .class ul.class_list {
        margin: 0 0 20px 0;
      }

    .item .name {
      background: #fafafa;
      margin: 0;
      font-weight: bold;
      padding: 5px 10px;
      border-radius: 3px;
      display: inline-block;
      min-width: 40%;
    }
      .item .name:hover {
        background: #f6f6f6;
      }

    .item .empty_desc {
      margin: 0 0 5px 0;
      padding: 0;
    }

    .item .inheritance {
      margin: 3px 0 0 30px;
    }

    .item .inherited {
      color: #666;
    }

    .item .desc {
      padding: 0 8px;
      margin: 0;
    }

      .item .desc p {
        margin: 0 0 10px 0;
      }

    .source_cont {
      margin: 0;
      padding: 0;
    }

    .source_link a {
      background: #ffc300;
      font-weight: 400;
      font-size: .75em;
      text-transform: uppercase;
      color: #fff;
      text-shadow: 1px 1px 0 #f4b700;
      
      padding: 3px 8px;
      border-radius: 2px;
      transition: background .3s ease-in-out;
    }
      .source_link a:hover {
        background: #FF7200;
        text-shadow: none;
        transition: background .3s ease-in-out;
      }

    .source {
      display: none;
      max-height: 600px;
      overflow-y: scroll;
      margin-bottom: 15px;
    }

      .source .codehilite {
        margin: 0;
      }

  .desc h1, .desc h2, .desc h3 {
    font-size: 100% !important;
  }
  .clear {
    clear: both;
  }

  @media all and (max-width: 950px) {
    #sidebar {
      width: 35%;
    }
    #content {
      width: 65%;
    }
  }
  @media all and (max-width: 650px) {
    #top {
      display: none;
    }
    #sidebar {
      float: none;
      width: auto;
    }
    #content {
      float: none;
      width: auto;
      padding: 30px;
    }

    #index ul {
      padding: 0;
      margin-bottom: 15px;
    }
    #index ul li {
      display: inline-block;
      margin-right: 30px;
    }
    #footer {
      text-align: left;
    }
    #footer p {
      display: block;
      margin: inherit;
    }
  }

  /*****************************/

  </style>

  <style type="text/css">
  .codehilite .hll { background-color: #ffffcc }
.codehilite  { background: #f8f8f8; }
.codehilite .c { color: #408080; font-style: italic } /* Comment */
.codehilite .err { border: 1px solid #FF0000 } /* Error */
.codehilite .k { color: #008000; font-weight: bold } /* Keyword */
.codehilite .o { color: #666666 } /* Operator */
.codehilite .ch { color: #408080; font-style: italic } /* Comment.Hashbang */
.codehilite .cm { color: #408080; font-style: italic } /* Comment.Multiline */
.codehilite .cp { color: #BC7A00 } /* Comment.Preproc */
.codehilite .cpf { color: #408080; font-style: italic } /* Comment.PreprocFile */
.codehilite .c1 { color: #408080; font-style: italic } /* Comment.Single */
.codehilite .cs { color: #408080; font-style: italic } /* Comment.Special */
.codehilite .gd { color: #A00000 } /* Generic.Deleted */
.codehilite .ge { font-style: italic } /* Generic.Emph */
.codehilite .gr { color: #FF0000 } /* Generic.Error */
.codehilite .gh { color: #000080; font-weight: bold } /* Generic.Heading */
.codehilite .gi { color: #00A000 } /* Generic.Inserted */
.codehilite .go { color: #888888 } /* Generic.Output */
.codehilite .gp { color: #000080; font-weight: bold } /* Generic.Prompt */
.codehilite .gs { font-weight: bold } /* Generic.Strong */
.codehilite .gu { color: #800080; font-weight: bold } /* Generic.Subheading */
.codehilite .gt { color: #0044DD } /* Generic.Traceback */
.codehilite .kc { color: #008000; font-weight: bold } /* Keyword.Constant */
.codehilite .kd { color: #008000; font-weight: bold } /* Keyword.Declaration */
.codehilite .kn { color: #008000; font-weight: bold } /* Keyword.Namespace */
.codehilite .kp { color: #008000 } /* Keyword.Pseudo */
.codehilite .kr { color: #008000; font-weight: bold } /* Keyword.Reserved */
.codehilite .kt { color: #B00040 } /* Keyword.Type */
.codehilite .m { color: #666666 } /* Literal.Number */
.codehilite .s { color: #BA2121 } /* Literal.String */
.codehilite .na { color: #7D9029 } /* Name.Attribute */
.codehilite .nb { color: #008000 } /* Name.Builtin */
.codehilite .nc { color: #0000FF; font-weight: bold } /* Name.Class */
.codehilite .no { color: #880000 } /* Name.Constant */
.codehilite .nd { color: #AA22FF } /* Name.Decorator */
.codehilite .ni { color: #999999; font-weight: bold } /* Name.Entity */
.codehilite .ne { color: #D2413A; font-weight: bold } /* Name.Exception */
.codehilite .nf { color: #0000FF } /* Name.Function */
.codehilite .nl { color: #A0A000 } /* Name.Label */
.codehilite .nn { color: #0000FF; font-weight: bold } /* Name.Namespace */
.codehilite .nt { color: #008000; font-weight: bold } /* Name.Tag */
.codehilite .nv { color: #19177C } /* Name.Variable */
.codehilite .ow { color: #AA22FF; font-weight: bold } /* Operator.Word */
.codehilite .w { color: #bbbbbb } /* Text.Whitespace */
.codehilite .mb { color: #666666 } /* Literal.Number.Bin */
.codehilite .mf { color: #666666 } /* Literal.Number.Float */
.codehilite .mh { color: #666666 } /* Literal.Number.Hex */
.codehilite .mi { color: #666666 } /* Literal.Number.Integer */
.codehilite .mo { color: #666666 } /* Literal.Number.Oct */
.codehilite .sa { color: #BA2121 } /* Literal.String.Affix */
.codehilite .sb { color: #BA2121 } /* Literal.String.Backtick */
.codehilite .sc { color: #BA2121 } /* Literal.String.Char */
.codehilite .dl { color: #BA2121 } /* Literal.String.Delimiter */
.codehilite .sd { color: #BA2121; font-style: italic } /* Literal.String.Doc */
.codehilite .s2 { color: #BA2121 } /* Literal.String.Double */
.codehilite .se { color: #BB6622; font-weight: bold } /* Literal.String.Escape */
.codehilite .sh { color: #BA2121 } /* Literal.String.Heredoc */
.codehilite .si { color: #BB6688; font-weight: bold } /* Literal.String.Interpol */
.codehilite .sx { color: #008000 } /* Literal.String.Other */
.codehilite .sr { color: #BB6688 } /* Literal.String.Regex */
.codehilite .s1 { color: #BA2121 } /* Literal.String.Single */
.codehilite .ss { color: #19177C } /* Literal.String.Symbol */
.codehilite .bp { color: #008000 } /* Name.Builtin.Pseudo */
.codehilite .fm { color: #0000FF } /* Name.Function.Magic */
.codehilite .vc { color: #19177C } /* Name.Variable.Class */
.codehilite .vg { color: #19177C } /* Name.Variable.Global */
.codehilite .vi { color: #19177C } /* Name.Variable.Instance */
.codehilite .vm { color: #19177C } /* Name.Variable.Magic */
.codehilite .il { color: #666666 } /* Literal.Number.Integer.Long */
  </style>

  <style type="text/css">
  
/* ==========================================================================
   EXAMPLE Media Queries for Responsive Design.
   These examples override the primary ('mobile first') styles.
   Modify as content requires.
   ========================================================================== */

@media only screen and (min-width: 35em) {
    /* Style adjustments for viewports that meet the condition */
}

@media print,
       (-o-min-device-pixel-ratio: 5/4),
       (-webkit-min-device-pixel-ratio: 1.25),
       (min-resolution: 120dpi) {
    /* Style adjustments for high resolution devices */
}

/* ==========================================================================
   Print styles.
   Inlined to avoid required HTTP connection: h5bp.com/r
   ========================================================================== */

@media print {
    * {
        background: transparent !important;
        color: #000 !important; /* Black prints faster: h5bp.com/s */
        box-shadow: none !important;
        text-shadow: none !important;
    }

    a,
    a:visited {
        text-decoration: underline;
    }

    a[href]:after {
        content: " (" attr(href) ")";
    }

    abbr[title]:after {
        content: " (" attr(title) ")";
    }

    /*
     * Don't show links for images, or javascript/internal links
     */

    .ir a:after,
    a[href^="javascript:"]:after,
    a[href^="#"]:after {
        content: "";
    }

    pre,
    blockquote {
        border: 1px solid #999;
        page-break-inside: avoid;
    }

    thead {
        display: table-header-group; /* h5bp.com/t */
    }

    tr,
    img {
        page-break-inside: avoid;
    }

    img {
        max-width: 100% !important;
    }

    @page {
        margin: 0.5cm;
    }

    p,
    h2,
    h3 {
        orphans: 3;
        widows: 3;
    }

    h2,
    h3 {
        page-break-after: avoid;
    }
}

  </style>

  <script type="text/javascript">
  function toggle(id, $link) {
    $node = document.getElementById(id);
    if (!$node)
    return;
    if (!$node.style.display || $node.style.display == 'none') {
    $node.style.display = 'block';
    $link.innerHTML = 'Hide source &nequiv;';
    } else {
    $node.style.display = 'none';
    $link.innerHTML = 'Show source &equiv;';
    }
  }
  </script>
</head>
<body>
<a href="#" id="top">Top</a>

<div id="container">
    
  
  <div id="sidebar">
    <h1>Index</h1>
    <ul id="index">


    <li class="set"><h3><a href="#header-classes">Classes</a></h3>
      <ul>
        <li class="mono">
        <span class="class_name"><a href="#latex_ocr.model.components.attention_cell.AttentionCell">AttentionCell</a></span>
        
          
  <ul>
    <li class="mono"><a href="#latex_ocr.model.components.attention_cell.AttentionCell.__init__">__init__</a></li>
    <li class="mono"><a href="#latex_ocr.model.components.attention_cell.AttentionCell.add_loss">add_loss</a></li>
    <li class="mono"><a href="#latex_ocr.model.components.attention_cell.AttentionCell.add_metric">add_metric</a></li>
    <li class="mono"><a href="#latex_ocr.model.components.attention_cell.AttentionCell.add_update">add_update</a></li>
    <li class="mono"><a href="#latex_ocr.model.components.attention_cell.AttentionCell.add_variable">add_variable</a></li>
    <li class="mono"><a href="#latex_ocr.model.components.attention_cell.AttentionCell.add_weight">add_weight</a></li>
    <li class="mono"><a href="#latex_ocr.model.components.attention_cell.AttentionCell.apply">apply</a></li>
    <li class="mono"><a href="#latex_ocr.model.components.attention_cell.AttentionCell.build">build</a></li>
    <li class="mono"><a href="#latex_ocr.model.components.attention_cell.AttentionCell.call">call</a></li>
    <li class="mono"><a href="#latex_ocr.model.components.attention_cell.AttentionCell.compute_mask">compute_mask</a></li>
    <li class="mono"><a href="#latex_ocr.model.components.attention_cell.AttentionCell.compute_output_shape">compute_output_shape</a></li>
    <li class="mono"><a href="#latex_ocr.model.components.attention_cell.AttentionCell.count_params">count_params</a></li>
    <li class="mono"><a href="#latex_ocr.model.components.attention_cell.AttentionCell.get_config">get_config</a></li>
    <li class="mono"><a href="#latex_ocr.model.components.attention_cell.AttentionCell.get_initial_state">get_initial_state</a></li>
    <li class="mono"><a href="#latex_ocr.model.components.attention_cell.AttentionCell.get_input_at">get_input_at</a></li>
    <li class="mono"><a href="#latex_ocr.model.components.attention_cell.AttentionCell.get_input_mask_at">get_input_mask_at</a></li>
    <li class="mono"><a href="#latex_ocr.model.components.attention_cell.AttentionCell.get_input_shape_at">get_input_shape_at</a></li>
    <li class="mono"><a href="#latex_ocr.model.components.attention_cell.AttentionCell.get_losses_for">get_losses_for</a></li>
    <li class="mono"><a href="#latex_ocr.model.components.attention_cell.AttentionCell.get_output_at">get_output_at</a></li>
    <li class="mono"><a href="#latex_ocr.model.components.attention_cell.AttentionCell.get_output_mask_at">get_output_mask_at</a></li>
    <li class="mono"><a href="#latex_ocr.model.components.attention_cell.AttentionCell.get_output_shape_at">get_output_shape_at</a></li>
    <li class="mono"><a href="#latex_ocr.model.components.attention_cell.AttentionCell.get_updates_for">get_updates_for</a></li>
    <li class="mono"><a href="#latex_ocr.model.components.attention_cell.AttentionCell.get_weights">get_weights</a></li>
    <li class="mono"><a href="#latex_ocr.model.components.attention_cell.AttentionCell.initial_state">initial_state</a></li>
    <li class="mono"><a href="#latex_ocr.model.components.attention_cell.AttentionCell.set_weights">set_weights</a></li>
    <li class="mono"><a href="#latex_ocr.model.components.attention_cell.AttentionCell.step">step</a></li>
    <li class="mono"><a href="#latex_ocr.model.components.attention_cell.AttentionCell.zero_state">zero_state</a></li>
    <li class="mono"><a href="#latex_ocr.model.components.attention_cell.AttentionCell.from_config">from_config</a></li>
  </ul>

        </li>
        <li class="mono">
        <span class="class_name"><a href="#latex_ocr.model.components.attention_cell.AttentionState">AttentionState</a></span>
        
        </li>
      </ul>
    </li>

    </ul>
  </div>

    <article id="content">
      
  

  


  <header id="section-intro">
  <h1 class="title"><span class="name">latex_ocr.model.components.attention_cell</span> module</h1>
  
  
  <p class="source_link"><a href="javascript:void(0);" onclick="toggle('source-latex_ocr.model.components.attention_cell', this);">Show source &equiv;</a></p>
  <div id="source-latex_ocr.model.components.attention_cell" class="source">
    <div class="codehilite"><pre><span></span><span class="kn">import</span> <span class="nn">tensorflow</span> <span class="kn">as</span> <span class="nn">tf</span>
<span class="kn">import</span> <span class="nn">collections</span>
<span class="kn">from</span> <span class="nn">tensorflow.contrib.rnn</span> <span class="kn">import</span> <span class="n">RNNCell</span><span class="p">,</span> <span class="n">LSTMStateTuple</span>


<span class="n">AttentionState</span> <span class="o">=</span> <span class="n">collections</span><span class="o">.</span><span class="n">namedtuple</span><span class="p">(</span><span class="s2">&quot;AttentionState&quot;</span><span class="p">,</span> <span class="p">(</span><span class="s2">&quot;cell_state&quot;</span><span class="p">,</span> <span class="s2">&quot;o&quot;</span><span class="p">))</span>


<span class="k">class</span> <span class="nc">AttentionCell</span><span class="p">(</span><span class="n">RNNCell</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">cell</span><span class="p">,</span> <span class="n">attention_mechanism</span><span class="p">,</span> <span class="n">dropout</span><span class="p">,</span> <span class="n">attn_cell_config</span><span class="p">,</span>
        <span class="n">num_proj</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Args:</span>
<span class="sd">            cell: (RNNCell)</span>
<span class="sd">            attention_mechanism: (AttentionMechanism)</span>
<span class="sd">            dropout: (tf.float)</span>
<span class="sd">            attn_cell_config: (dict) hyper params</span>

<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># variables and tensors</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_cell</span>                <span class="o">=</span> <span class="n">cell</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_attention_mechanism</span> <span class="o">=</span> <span class="n">attention_mechanism</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_dropout</span>             <span class="o">=</span> <span class="n">dropout</span>

        <span class="c1"># hyperparameters and shapes</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_n_channels</span>     <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_attention_mechanism</span><span class="o">.</span><span class="n">_n_channels</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_dim_e</span>          <span class="o">=</span> <span class="n">attn_cell_config</span><span class="p">[</span><span class="s2">&quot;dim_e&quot;</span><span class="p">]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_dim_o</span>          <span class="o">=</span> <span class="n">attn_cell_config</span><span class="p">[</span><span class="s2">&quot;dim_o&quot;</span><span class="p">]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_num_units</span>      <span class="o">=</span> <span class="n">attn_cell_config</span><span class="p">[</span><span class="s2">&quot;num_units&quot;</span><span class="p">]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_dim_embeddings</span> <span class="o">=</span> <span class="n">attn_cell_config</span><span class="p">[</span><span class="s2">&quot;dim_embeddings&quot;</span><span class="p">]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_num_proj</span>       <span class="o">=</span> <span class="n">num_proj</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_dtype</span>          <span class="o">=</span> <span class="n">dtype</span>

        <span class="c1"># for RNNCell</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_state_size</span> <span class="o">=</span> <span class="n">AttentionState</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_cell</span><span class="o">.</span><span class="n">_state_size</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_dim_o</span><span class="p">)</span>


    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">state_size</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_state_size</span>


    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">output_size</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_num_proj</span>


    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">output_dtype</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_dtype</span>


    <span class="k">def</span> <span class="nf">initial_state</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Returns initial state for the lstm&quot;&quot;&quot;</span>
        <span class="n">initial_cell_state</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_attention_mechanism</span><span class="o">.</span><span class="n">initial_cell_state</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_cell</span><span class="p">)</span>
        <span class="n">initial_o</span>          <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_attention_mechanism</span><span class="o">.</span><span class="n">initial_state</span><span class="p">(</span><span class="s2">&quot;o&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_dim_o</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">AttentionState</span><span class="p">(</span><span class="n">initial_cell_state</span><span class="p">,</span> <span class="n">initial_o</span><span class="p">)</span>


    <span class="k">def</span> <span class="nf">step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">embedding</span><span class="p">,</span> <span class="n">attn_cell_state</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Args:</span>
<span class="sd">            embedding: shape = (batch_size, dim_embeddings) embeddings</span>
<span class="sd">                from previous time step</span>
<span class="sd">            attn_cell_state: (AttentionState) state from previous time step</span>

<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">prev_cell_state</span><span class="p">,</span> <span class="n">o</span> <span class="o">=</span> <span class="n">attn_cell_state</span>

        <span class="n">scope</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">get_variable_scope</span><span class="p">()</span>
        <span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">variable_scope</span><span class="p">(</span><span class="n">scope</span><span class="p">):</span>
            <span class="c1"># compute new h</span>
            <span class="n">x</span>                     <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">concat</span><span class="p">([</span><span class="n">embedding</span><span class="p">,</span> <span class="n">o</span><span class="p">],</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
            <span class="n">new_h</span><span class="p">,</span> <span class="n">new_cell_state</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_cell</span><span class="o">.</span><span class="fm">__call__</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">prev_cell_state</span><span class="p">)</span>
            <span class="n">new_h</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">dropout</span><span class="p">(</span><span class="n">new_h</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_dropout</span><span class="p">)</span>

            <span class="c1"># compute attention</span>
            <span class="n">c</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_attention_mechanism</span><span class="o">.</span><span class="n">context</span><span class="p">(</span><span class="n">new_h</span><span class="p">)</span>

            <span class="c1"># compute o</span>
            <span class="n">o_W_c</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">get_variable</span><span class="p">(</span><span class="s2">&quot;o_W_c&quot;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span>
                    <span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_n_channels</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_dim_o</span><span class="p">))</span>
            <span class="n">o_W_h</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">get_variable</span><span class="p">(</span><span class="s2">&quot;o_W_h&quot;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span>
                    <span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_num_units</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_dim_o</span><span class="p">))</span>

            <span class="n">new_o</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">tanh</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">new_h</span><span class="p">,</span> <span class="n">o_W_h</span><span class="p">)</span> <span class="o">+</span> <span class="n">tf</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">c</span><span class="p">,</span> <span class="n">o_W_c</span><span class="p">))</span>
            <span class="n">new_o</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">dropout</span><span class="p">(</span><span class="n">new_o</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_dropout</span><span class="p">)</span>

            <span class="n">y_W_o</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">get_variable</span><span class="p">(</span><span class="s2">&quot;y_W_o&quot;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span>
                    <span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_dim_o</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_num_proj</span><span class="p">))</span>
            <span class="n">logits</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">new_o</span><span class="p">,</span> <span class="n">y_W_o</span><span class="p">)</span>

            <span class="c1"># new Attn cell state</span>
            <span class="n">new_state</span> <span class="o">=</span> <span class="n">AttentionState</span><span class="p">(</span><span class="n">new_cell_state</span><span class="p">,</span> <span class="n">new_o</span><span class="p">)</span>

            <span class="k">return</span> <span class="n">logits</span><span class="p">,</span> <span class="n">new_state</span>


    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">state</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Args:</span>
<span class="sd">            inputs: the embedding of the previous word for training only</span>
<span class="sd">            state: (AttentionState) (h, o) where h is the hidden state and</span>
<span class="sd">                o is the vector used to make the prediction of</span>
<span class="sd">                the previous word</span>

<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">new_output</span><span class="p">,</span> <span class="n">new_state</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">state</span><span class="p">)</span>

        <span class="k">return</span> <span class="p">(</span><span class="n">new_output</span><span class="p">,</span> <span class="n">new_state</span><span class="p">)</span>
</pre></div>

  </div>

  </header>

  <section id="section-items">


    <h2 class="section-title" id="header-classes">Classes</h2>
      
      <div class="item">
      <p id="latex_ocr.model.components.attention_cell.AttentionCell" class="name">class <span class="ident">AttentionCell</span></p>
      
  
    <div class="desc"><p>Abstract object representing an RNN cell.</p>
<p>Every <code>RNNCell</code> must have the properties below and implement <code>call</code> with
the signature <code>(output, next_state) = call(input, state)</code>.  The optional
third input argument, <code>scope</code>, is allowed for backwards compatibility
purposes; but should be left off for new subclasses.</p>
<p>This definition of cell differs from the definition used in the literature.
In the literature, 'cell' refers to an object with a single scalar output.
This definition refers to a horizontal array of such units.</p>
<p>An RNN cell, in the most abstract setting, is anything that has
a state and performs some operation that takes a matrix of inputs.
This operation results in an output matrix with <code>self.output_size</code> columns.
If <code>self.state_size</code> is an integer, this operation also results in a new
state matrix with <code>self.state_size</code> columns.  If <code>self.state_size</code> is a
(possibly nested tuple of) TensorShape object(s), then it should return a
matching structure of Tensors having shape <code>[batch_size].concatenate(s)</code>
for each <code>s</code> in <code>self.batch_size</code>.</p></div>
  <div class="source_cont">
  <p class="source_link"><a href="javascript:void(0);" onclick="toggle('source-latex_ocr.model.components.attention_cell.AttentionCell', this);">Show source &equiv;</a></p>
  <div id="source-latex_ocr.model.components.attention_cell.AttentionCell" class="source">
    <div class="codehilite"><pre><span></span><span class="k">class</span> <span class="nc">AttentionCell</span><span class="p">(</span><span class="n">RNNCell</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">cell</span><span class="p">,</span> <span class="n">attention_mechanism</span><span class="p">,</span> <span class="n">dropout</span><span class="p">,</span> <span class="n">attn_cell_config</span><span class="p">,</span>
        <span class="n">num_proj</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Args:</span>
<span class="sd">            cell: (RNNCell)</span>
<span class="sd">            attention_mechanism: (AttentionMechanism)</span>
<span class="sd">            dropout: (tf.float)</span>
<span class="sd">            attn_cell_config: (dict) hyper params</span>

<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># variables and tensors</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_cell</span>                <span class="o">=</span> <span class="n">cell</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_attention_mechanism</span> <span class="o">=</span> <span class="n">attention_mechanism</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_dropout</span>             <span class="o">=</span> <span class="n">dropout</span>

        <span class="c1"># hyperparameters and shapes</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_n_channels</span>     <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_attention_mechanism</span><span class="o">.</span><span class="n">_n_channels</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_dim_e</span>          <span class="o">=</span> <span class="n">attn_cell_config</span><span class="p">[</span><span class="s2">&quot;dim_e&quot;</span><span class="p">]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_dim_o</span>          <span class="o">=</span> <span class="n">attn_cell_config</span><span class="p">[</span><span class="s2">&quot;dim_o&quot;</span><span class="p">]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_num_units</span>      <span class="o">=</span> <span class="n">attn_cell_config</span><span class="p">[</span><span class="s2">&quot;num_units&quot;</span><span class="p">]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_dim_embeddings</span> <span class="o">=</span> <span class="n">attn_cell_config</span><span class="p">[</span><span class="s2">&quot;dim_embeddings&quot;</span><span class="p">]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_num_proj</span>       <span class="o">=</span> <span class="n">num_proj</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_dtype</span>          <span class="o">=</span> <span class="n">dtype</span>

        <span class="c1"># for RNNCell</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_state_size</span> <span class="o">=</span> <span class="n">AttentionState</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_cell</span><span class="o">.</span><span class="n">_state_size</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_dim_o</span><span class="p">)</span>


    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">state_size</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_state_size</span>


    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">output_size</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_num_proj</span>


    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">output_dtype</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_dtype</span>


    <span class="k">def</span> <span class="nf">initial_state</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Returns initial state for the lstm&quot;&quot;&quot;</span>
        <span class="n">initial_cell_state</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_attention_mechanism</span><span class="o">.</span><span class="n">initial_cell_state</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_cell</span><span class="p">)</span>
        <span class="n">initial_o</span>          <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_attention_mechanism</span><span class="o">.</span><span class="n">initial_state</span><span class="p">(</span><span class="s2">&quot;o&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_dim_o</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">AttentionState</span><span class="p">(</span><span class="n">initial_cell_state</span><span class="p">,</span> <span class="n">initial_o</span><span class="p">)</span>


    <span class="k">def</span> <span class="nf">step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">embedding</span><span class="p">,</span> <span class="n">attn_cell_state</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Args:</span>
<span class="sd">            embedding: shape = (batch_size, dim_embeddings) embeddings</span>
<span class="sd">                from previous time step</span>
<span class="sd">            attn_cell_state: (AttentionState) state from previous time step</span>

<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">prev_cell_state</span><span class="p">,</span> <span class="n">o</span> <span class="o">=</span> <span class="n">attn_cell_state</span>

        <span class="n">scope</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">get_variable_scope</span><span class="p">()</span>
        <span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">variable_scope</span><span class="p">(</span><span class="n">scope</span><span class="p">):</span>
            <span class="c1"># compute new h</span>
            <span class="n">x</span>                     <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">concat</span><span class="p">([</span><span class="n">embedding</span><span class="p">,</span> <span class="n">o</span><span class="p">],</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
            <span class="n">new_h</span><span class="p">,</span> <span class="n">new_cell_state</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_cell</span><span class="o">.</span><span class="fm">__call__</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">prev_cell_state</span><span class="p">)</span>
            <span class="n">new_h</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">dropout</span><span class="p">(</span><span class="n">new_h</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_dropout</span><span class="p">)</span>

            <span class="c1"># compute attention</span>
            <span class="n">c</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_attention_mechanism</span><span class="o">.</span><span class="n">context</span><span class="p">(</span><span class="n">new_h</span><span class="p">)</span>

            <span class="c1"># compute o</span>
            <span class="n">o_W_c</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">get_variable</span><span class="p">(</span><span class="s2">&quot;o_W_c&quot;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span>
                    <span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_n_channels</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_dim_o</span><span class="p">))</span>
            <span class="n">o_W_h</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">get_variable</span><span class="p">(</span><span class="s2">&quot;o_W_h&quot;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span>
                    <span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_num_units</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_dim_o</span><span class="p">))</span>

            <span class="n">new_o</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">tanh</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">new_h</span><span class="p">,</span> <span class="n">o_W_h</span><span class="p">)</span> <span class="o">+</span> <span class="n">tf</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">c</span><span class="p">,</span> <span class="n">o_W_c</span><span class="p">))</span>
            <span class="n">new_o</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">dropout</span><span class="p">(</span><span class="n">new_o</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_dropout</span><span class="p">)</span>

            <span class="n">y_W_o</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">get_variable</span><span class="p">(</span><span class="s2">&quot;y_W_o&quot;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span>
                    <span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_dim_o</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_num_proj</span><span class="p">))</span>
            <span class="n">logits</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">new_o</span><span class="p">,</span> <span class="n">y_W_o</span><span class="p">)</span>

            <span class="c1"># new Attn cell state</span>
            <span class="n">new_state</span> <span class="o">=</span> <span class="n">AttentionState</span><span class="p">(</span><span class="n">new_cell_state</span><span class="p">,</span> <span class="n">new_o</span><span class="p">)</span>

            <span class="k">return</span> <span class="n">logits</span><span class="p">,</span> <span class="n">new_state</span>


    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">state</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Args:</span>
<span class="sd">            inputs: the embedding of the previous word for training only</span>
<span class="sd">            state: (AttentionState) (h, o) where h is the hidden state and</span>
<span class="sd">                o is the vector used to make the prediction of</span>
<span class="sd">                the previous word</span>

<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">new_output</span><span class="p">,</span> <span class="n">new_state</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">state</span><span class="p">)</span>

        <span class="k">return</span> <span class="p">(</span><span class="n">new_output</span><span class="p">,</span> <span class="n">new_state</span><span class="p">)</span>
</pre></div>

  </div>
</div>


      <div class="class">
          <h3>Ancestors (in MRO)</h3>
          <ul class="class_list">
          <li><a href="#latex_ocr.model.components.attention_cell.AttentionCell">AttentionCell</a></li>
          <li>tensorflow.python.ops.rnn_cell_impl.RNNCell</li>
          <li>tensorflow.python.layers.base.Layer</li>
          <li>tensorflow.python.keras.engine.base_layer.Layer</li>
          <li>tensorflow.python.training.checkpointable.base.CheckpointableBase</li>
          <li>builtins.object</li>
          </ul>
          <h3>Static methods</h3>
            
  <div class="item">
    <div class="name def" id="latex_ocr.model.components.attention_cell.AttentionCell.__init__">
    <p>def <span class="ident">__init__</span>(</p><p>self, cell, attention_mechanism, dropout, attn_cell_config, num_proj, dtype=tf.float32)</p>
    </div>
    

    
  
    <div class="desc"><p>Args:
cell: (RNNCell)
attention_mechanism: (AttentionMechanism)
dropout: (tf.float)
attn_cell_config: (dict) hyper params</p></div>
  <div class="source_cont">
  <p class="source_link"><a href="javascript:void(0);" onclick="toggle('source-latex_ocr.model.components.attention_cell.AttentionCell.__init__', this);">Show source &equiv;</a></p>
  <div id="source-latex_ocr.model.components.attention_cell.AttentionCell.__init__" class="source">
    <div class="codehilite"><pre><span></span><span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">cell</span><span class="p">,</span> <span class="n">attention_mechanism</span><span class="p">,</span> <span class="n">dropout</span><span class="p">,</span> <span class="n">attn_cell_config</span><span class="p">,</span>
    <span class="n">num_proj</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Args:</span>
<span class="sd">        cell: (RNNCell)</span>
<span class="sd">        attention_mechanism: (AttentionMechanism)</span>
<span class="sd">        dropout: (tf.float)</span>
<span class="sd">        attn_cell_config: (dict) hyper params</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># variables and tensors</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_cell</span>                <span class="o">=</span> <span class="n">cell</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_attention_mechanism</span> <span class="o">=</span> <span class="n">attention_mechanism</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_dropout</span>             <span class="o">=</span> <span class="n">dropout</span>
    <span class="c1"># hyperparameters and shapes</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_n_channels</span>     <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_attention_mechanism</span><span class="o">.</span><span class="n">_n_channels</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_dim_e</span>          <span class="o">=</span> <span class="n">attn_cell_config</span><span class="p">[</span><span class="s2">&quot;dim_e&quot;</span><span class="p">]</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_dim_o</span>          <span class="o">=</span> <span class="n">attn_cell_config</span><span class="p">[</span><span class="s2">&quot;dim_o&quot;</span><span class="p">]</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_num_units</span>      <span class="o">=</span> <span class="n">attn_cell_config</span><span class="p">[</span><span class="s2">&quot;num_units&quot;</span><span class="p">]</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_dim_embeddings</span> <span class="o">=</span> <span class="n">attn_cell_config</span><span class="p">[</span><span class="s2">&quot;dim_embeddings&quot;</span><span class="p">]</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_num_proj</span>       <span class="o">=</span> <span class="n">num_proj</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_dtype</span>          <span class="o">=</span> <span class="n">dtype</span>
    <span class="c1"># for RNNCell</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_state_size</span> <span class="o">=</span> <span class="n">AttentionState</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_cell</span><span class="o">.</span><span class="n">_state_size</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_dim_o</span><span class="p">)</span>
</pre></div>

  </div>
</div>

  </div>
  
            
  <div class="item">
    <div class="name def" id="latex_ocr.model.components.attention_cell.AttentionCell.add_loss">
    <p>def <span class="ident">add_loss</span>(</p><p>self, losses, inputs=None)</p>
    </div>
    

    
  
    <div class="desc"><p>Add loss tensor(s), potentially dependent on layer inputs.</p>
<p>Some losses (for instance, activity regularization losses) may be dependent
on the inputs passed when calling a layer. Hence, when reusing the same
layer on different inputs <code>a</code> and <code>b</code>, some entries in <code>layer.losses</code> may
be dependent on <code>a</code> and some on <code>b</code>. This method automatically keeps track
of dependencies.</p>
<p>The <code>get_losses_for</code> method allows to retrieve the losses relevant to a
specific set of inputs.</p>
<p>Note that <code>add_loss</code> is not supported when executing eagerly. Instead,
variable regularizers may be added through <code>add_variable</code>. Activity
regularization is not supported directly (but such losses may be returned
from <code>Layer.call()</code>).</p>
<p>Arguments:
  losses: Loss tensor, or list/tuple of tensors. Rather than tensors, losses
    may also be zero-argument callables which create a loss tensor.
  inputs: Ignored when executing eagerly. If anything other than None is
    passed, it signals the losses are conditional on some of the layer's
    inputs, and thus they should only be run where these inputs are
    available. This is the case for activity regularization losses, for
    instance. If <code>None</code> is passed, the losses are assumed
    to be unconditional, and will apply across all dataflows of the layer
    (e.g. weight regularization losses).</p></div>
  <div class="source_cont">
  <p class="source_link"><a href="javascript:void(0);" onclick="toggle('source-latex_ocr.model.components.attention_cell.AttentionCell.add_loss', this);">Show source &equiv;</a></p>
  <div id="source-latex_ocr.model.components.attention_cell.AttentionCell.add_loss" class="source">
    <div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">add_loss</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">losses</span><span class="p">,</span> <span class="n">inputs</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
  <span class="n">previous_losses_length</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_losses</span><span class="p">)</span>
  <span class="n">previous_callable_losses_length</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_callable_losses</span><span class="p">)</span>
  <span class="nb">super</span><span class="p">(</span><span class="n">Layer</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="n">add_loss</span><span class="p">(</span><span class="n">losses</span><span class="p">,</span> <span class="n">inputs</span><span class="o">=</span><span class="n">inputs</span><span class="p">)</span>
  <span class="k">if</span> <span class="ow">not</span> <span class="n">context</span><span class="o">.</span><span class="n">executing_eagerly</span><span class="p">():</span>
    <span class="c1"># TODO(fchollet): deprecate collection below.</span>
    <span class="n">new_losses</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_losses</span><span class="p">[</span><span class="n">previous_losses_length</span><span class="p">:]</span>
    <span class="n">new_callable_losses</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_callable_losses</span><span class="p">[</span>
        <span class="n">previous_callable_losses_length</span><span class="p">:]</span>
    <span class="k">for</span> <span class="n">regularizer</span> <span class="ow">in</span> <span class="n">new_callable_losses</span><span class="p">:</span>
      <span class="n">loss_tensor</span> <span class="o">=</span> <span class="n">regularizer</span><span class="p">()</span>
      <span class="k">if</span> <span class="n">loss_tensor</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span><span class="p">:</span>
        <span class="n">new_losses</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">loss_tensor</span><span class="p">)</span>
    <span class="n">_add_elements_to_collection</span><span class="p">(</span>
        <span class="n">new_losses</span><span class="p">,</span>
        <span class="n">ops</span><span class="o">.</span><span class="n">GraphKeys</span><span class="o">.</span><span class="n">REGULARIZATION_LOSSES</span><span class="p">)</span>
</pre></div>

  </div>
</div>

  </div>
  
            
  <div class="item">
    <div class="name def" id="latex_ocr.model.components.attention_cell.AttentionCell.add_metric">
    <p>def <span class="ident">add_metric</span>(</p><p>self, value, aggregation=None, name=None)</p>
    </div>
    

    
  
    <div class="desc"><p>Adds metric tensor to the layer.</p>
<p>Args:
  value: Metric tensor.
  aggregation: Sample-wise metric reduction function. If <code>aggregation=None</code>,
    it indicates that the metric tensor provided has been aggregated
    already. eg, <code>model.add_metric(BinaryAccuracy(name='acc')(y_true,
    y_pred))</code>. If aggregation='mean', the given metric tensor will be
    sample-wise reduced using <code>mean</code> function. eg, <code>model.add_metric(
    tf.reduce_mean(outputs), name='output_mean', aggregation='mean')</code>.
  name: String metric name.</p>
<p>Raises:
  ValueError: If <code>aggregation</code> is anything other than None or <code>mean</code>.</p></div>
  <div class="source_cont">
  <p class="source_link"><a href="javascript:void(0);" onclick="toggle('source-latex_ocr.model.components.attention_cell.AttentionCell.add_metric', this);">Show source &equiv;</a></p>
  <div id="source-latex_ocr.model.components.attention_cell.AttentionCell.add_metric" class="source">
    <div class="codehilite"><pre><span></span><span class="nd">@doc_controls.for_subclass_implementers</span>
<span class="k">def</span> <span class="nf">add_metric</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">value</span><span class="p">,</span> <span class="n">aggregation</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Adds metric tensor to the layer.</span>
<span class="sd">  Args:</span>
<span class="sd">    value: Metric tensor.</span>
<span class="sd">    aggregation: Sample-wise metric reduction function. If `aggregation=None`,</span>
<span class="sd">      it indicates that the metric tensor provided has been aggregated</span>
<span class="sd">      already. eg, `model.add_metric(BinaryAccuracy(name=&#39;acc&#39;)(y_true,</span>
<span class="sd">      y_pred))`. If aggregation=&#39;mean&#39;, the given metric tensor will be</span>
<span class="sd">      sample-wise reduced using `mean` function. eg, `model.add_metric(</span>
<span class="sd">      tf.reduce_mean(outputs), name=&#39;output_mean&#39;, aggregation=&#39;mean&#39;)`.</span>
<span class="sd">    name: String metric name.</span>
<span class="sd">  Raises:</span>
<span class="sd">    ValueError: If `aggregation` is anything other than None or `mean`.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="k">if</span> <span class="n">aggregation</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span> <span class="ow">and</span> <span class="n">aggregation</span> <span class="o">!=</span> <span class="s1">&#39;mean&#39;</span><span class="p">:</span>
    <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
        <span class="s1">&#39;We currently support only `mean` sample-wise metric aggregation. &#39;</span>
        <span class="s1">&#39;You provided aggregation=`</span><span class="si">%s</span><span class="s1">`&#39;</span> <span class="o">%</span> <span class="n">aggregation</span><span class="p">)</span>
  <span class="k">if</span> <span class="n">tf_utils</span><span class="o">.</span><span class="n">is_symbolic_tensor</span><span class="p">(</span><span class="n">value</span><span class="p">):</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_symbolic_add_metric</span><span class="p">(</span><span class="n">value</span><span class="p">,</span> <span class="n">aggregation</span><span class="p">,</span> <span class="n">name</span><span class="p">)</span>
  <span class="k">else</span><span class="p">:</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_eager_add_metric</span><span class="p">(</span><span class="n">value</span><span class="p">,</span> <span class="n">aggregation</span><span class="p">,</span> <span class="n">name</span><span class="p">)</span>
</pre></div>

  </div>
</div>

  </div>
  
            
  <div class="item">
    <div class="name def" id="latex_ocr.model.components.attention_cell.AttentionCell.add_update">
    <p>def <span class="ident">add_update</span>(</p><p>self, updates, inputs=None)</p>
    </div>
    

    
  
    <div class="desc"><p>Add update op(s), potentially dependent on layer inputs.</p>
<p>Weight updates (for instance, the updates of the moving mean and variance
in a BatchNormalization layer) may be dependent on the inputs passed
when calling a layer. Hence, when reusing the same layer on
different inputs <code>a</code> and <code>b</code>, some entries in <code>layer.updates</code> may be
dependent on <code>a</code> and some on <code>b</code>. This method automatically keeps track
of dependencies.</p>
<p>The <code>get_updates_for</code> method allows to retrieve the updates relevant to a
specific set of inputs.</p>
<p>This call is ignored when eager execution is enabled (in that case, variable
updates are run on the fly and thus do not need to be tracked for later
execution).</p>
<p>Arguments:
  updates: Update op, or list/tuple of update ops.
  inputs: If anything other than None is passed, it signals the updates
    are conditional on some of the layer's inputs,
    and thus they should only be run where these inputs are available.
    This is the case for BatchNormalization updates, for instance.
    If None, the updates will be taken into account unconditionally,
    and you are responsible for making sure that any dependency they might
    have is available at runtime.
    A step counter might fall into this category.</p></div>
  <div class="source_cont">
  <p class="source_link"><a href="javascript:void(0);" onclick="toggle('source-latex_ocr.model.components.attention_cell.AttentionCell.add_update', this);">Show source &equiv;</a></p>
  <div id="source-latex_ocr.model.components.attention_cell.AttentionCell.add_update" class="source">
    <div class="codehilite"><pre><span></span><span class="nd">@doc_controls.for_subclass_implementers</span>
<span class="k">def</span> <span class="nf">add_update</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">updates</span><span class="p">,</span> <span class="n">inputs</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Add update op(s), potentially dependent on layer inputs.</span>
<span class="sd">  Weight updates (for instance, the updates of the moving mean and variance</span>
<span class="sd">  in a BatchNormalization layer) may be dependent on the inputs passed</span>
<span class="sd">  when calling a layer. Hence, when reusing the same layer on</span>
<span class="sd">  different inputs `a` and `b`, some entries in `layer.updates` may be</span>
<span class="sd">  dependent on `a` and some on `b`. This method automatically keeps track</span>
<span class="sd">  of dependencies.</span>
<span class="sd">  The `get_updates_for` method allows to retrieve the updates relevant to a</span>
<span class="sd">  specific set of inputs.</span>
<span class="sd">  This call is ignored when eager execution is enabled (in that case, variable</span>
<span class="sd">  updates are run on the fly and thus do not need to be tracked for later</span>
<span class="sd">  execution).</span>
<span class="sd">  Arguments:</span>
<span class="sd">    updates: Update op, or list/tuple of update ops.</span>
<span class="sd">    inputs: If anything other than None is passed, it signals the updates</span>
<span class="sd">      are conditional on some of the layer&#39;s inputs,</span>
<span class="sd">      and thus they should only be run where these inputs are available.</span>
<span class="sd">      This is the case for BatchNormalization updates, for instance.</span>
<span class="sd">      If None, the updates will be taken into account unconditionally,</span>
<span class="sd">      and you are responsible for making sure that any dependency they might</span>
<span class="sd">      have is available at runtime.</span>
<span class="sd">      A step counter might fall into this category.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="k">if</span> <span class="n">context</span><span class="o">.</span><span class="n">executing_eagerly</span><span class="p">():</span>
    <span class="k">return</span>  <span class="c1"># Updates already applied when in eager mode.</span>
  <span class="k">def</span> <span class="nf">process_update</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">ops</span><span class="o">.</span><span class="n">Operation</span><span class="p">):</span>
      <span class="k">return</span> <span class="n">x</span>
    <span class="k">elif</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="s1">&#39;op&#39;</span><span class="p">):</span>
      <span class="k">return</span> <span class="n">x</span><span class="o">.</span><span class="n">op</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="k">return</span> <span class="n">ops</span><span class="o">.</span><span class="n">convert_to_tensor</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
  <span class="n">updates</span> <span class="o">=</span> <span class="n">generic_utils</span><span class="o">.</span><span class="n">to_list</span><span class="p">(</span><span class="n">updates</span><span class="p">)</span>
  <span class="n">updates</span> <span class="o">=</span> <span class="p">[</span><span class="n">process_update</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">updates</span><span class="p">]</span>
  <span class="bp">self</span><span class="o">.</span><span class="n">_updates</span> <span class="o">+=</span> <span class="n">updates</span>
  <span class="k">if</span> <span class="n">inputs</span> <span class="ow">is</span> <span class="bp">None</span><span class="p">:</span>
    <span class="k">for</span> <span class="n">u</span> <span class="ow">in</span> <span class="n">updates</span><span class="p">:</span>
      <span class="n">u</span><span class="o">.</span><span class="n">_unconditional_update</span> <span class="o">=</span> <span class="bp">True</span>  <span class="c1"># pylint: disable=protected-access</span>
  <span class="k">else</span><span class="p">:</span>
    <span class="k">for</span> <span class="n">u</span> <span class="ow">in</span> <span class="n">updates</span><span class="p">:</span>
      <span class="n">u</span><span class="o">.</span><span class="n">_unconditional_update</span> <span class="o">=</span> <span class="bp">False</span>  <span class="c1"># pylint: disable=protected-access</span>
</pre></div>

  </div>
</div>

  </div>
  
            
  <div class="item">
    <div class="name def" id="latex_ocr.model.components.attention_cell.AttentionCell.add_variable">
    <p>def <span class="ident">add_variable</span>(</p><p>self, *args, **kwargs)</p>
    </div>
    

    
  
    <div class="desc"><p>Alias for <code>add_weight</code>.</p></div>
  <div class="source_cont">
  <p class="source_link"><a href="javascript:void(0);" onclick="toggle('source-latex_ocr.model.components.attention_cell.AttentionCell.add_variable', this);">Show source &equiv;</a></p>
  <div id="source-latex_ocr.model.components.attention_cell.AttentionCell.add_variable" class="source">
    <div class="codehilite"><pre><span></span><span class="nd">@doc_controls.for_subclass_implementers</span>
<span class="k">def</span> <span class="nf">add_variable</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Alias for `add_weight`.&quot;&quot;&quot;</span>
  <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">add_weight</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
</pre></div>

  </div>
</div>

  </div>
  
            
  <div class="item">
    <div class="name def" id="latex_ocr.model.components.attention_cell.AttentionCell.add_weight">
    <p>def <span class="ident">add_weight</span>(</p><p>self, name, shape, dtype=None, initializer=None, regularizer=None, trainable=None, constraint=None, use_resource=None, synchronization=&lt;VariableSynchronization.AUTO: 0&gt;, aggregation=&lt;VariableAggregation.NONE: 0&gt;, partitioner=None)</p>
    </div>
    

    
  
    <div class="desc"><p>Adds a new variable to the layer, or gets an existing one; returns it.</p>
<p>Arguments:
  name: variable name.
  shape: variable shape.
  dtype: The type of the variable. Defaults to <code>self.dtype</code> or <code>float32</code>.
  initializer: initializer instance (callable).
  regularizer: regularizer instance (callable).
  trainable: whether the variable should be part of the layer's
    "trainable_variables" (e.g. variables, biases)
    or "non_trainable_variables" (e.g. BatchNorm mean, stddev).
    Note, if the current variable scope is marked as non-trainable
    then this parameter is ignored and any added variables are also
    marked as non-trainable. <code>trainable</code> defaults to <code>True</code> unless
    <code>synchronization</code> is set to <code>ON_READ</code>.
  constraint: constraint instance (callable).
  use_resource: Whether to use <code>ResourceVariable</code>.
  synchronization: Indicates when a distributed a variable will be
    aggregated. Accepted values are constants defined in the class
    <code>tf.VariableSynchronization</code>. By default the synchronization is set to
    <code>AUTO</code> and the current <code>DistributionStrategy</code> chooses
    when to synchronize. If <code>synchronization</code> is set to <code>ON_READ</code>,
    <code>trainable</code> must not be set to <code>True</code>.
  aggregation: Indicates how a distributed variable will be aggregated.
    Accepted values are constants defined in the class
    <code>tf.VariableAggregation</code>.
  partitioner: (optional) partitioner instance (callable).  If
    provided, when the requested variable is created it will be split
    into multiple partitions according to <code>partitioner</code>.  In this case,
    an instance of <code>PartitionedVariable</code> is returned.  Available
    partitioners include <code>tf.fixed_size_partitioner</code> and
    <code>tf.variable_axis_size_partitioner</code>.  For more details, see the
    documentation of <code>tf.get_variable</code> and the  "Variable Partitioners
    and Sharding" section of the API guide.</p>
<p>Returns:
  The created variable.  Usually either a <code>Variable</code> or <code>ResourceVariable</code>
  instance.  If <code>partitioner</code> is not <code>None</code>, a <code>PartitionedVariable</code>
  instance is returned.</p>
<p>Raises:
  RuntimeError: If called with partioned variable regularization and
    eager execution is enabled.
  ValueError: When trainable has been set to True with synchronization
    set as <code>ON_READ</code>.</p></div>
  <div class="source_cont">
  <p class="source_link"><a href="javascript:void(0);" onclick="toggle('source-latex_ocr.model.components.attention_cell.AttentionCell.add_weight', this);">Show source &equiv;</a></p>
  <div id="source-latex_ocr.model.components.attention_cell.AttentionCell.add_weight" class="source">
    <div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">add_weight</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
               <span class="n">name</span><span class="p">,</span>
               <span class="n">shape</span><span class="p">,</span>
               <span class="n">dtype</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span>
               <span class="n">initializer</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span>
               <span class="n">regularizer</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span>
               <span class="n">trainable</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span>
               <span class="n">constraint</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span>
               <span class="n">use_resource</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span>
               <span class="n">synchronization</span><span class="o">=</span><span class="n">vs</span><span class="o">.</span><span class="n">VariableSynchronization</span><span class="o">.</span><span class="n">AUTO</span><span class="p">,</span>
               <span class="n">aggregation</span><span class="o">=</span><span class="n">vs</span><span class="o">.</span><span class="n">VariableAggregation</span><span class="o">.</span><span class="n">NONE</span><span class="p">,</span>
               <span class="n">partitioner</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Adds a new variable to the layer, or gets an existing one; returns it.</span>
<span class="sd">  Arguments:</span>
<span class="sd">    name: variable name.</span>
<span class="sd">    shape: variable shape.</span>
<span class="sd">    dtype: The type of the variable. Defaults to `self.dtype` or `float32`.</span>
<span class="sd">    initializer: initializer instance (callable).</span>
<span class="sd">    regularizer: regularizer instance (callable).</span>
<span class="sd">    trainable: whether the variable should be part of the layer&#39;s</span>
<span class="sd">      &quot;trainable_variables&quot; (e.g. variables, biases)</span>
<span class="sd">      or &quot;non_trainable_variables&quot; (e.g. BatchNorm mean, stddev).</span>
<span class="sd">      Note, if the current variable scope is marked as non-trainable</span>
<span class="sd">      then this parameter is ignored and any added variables are also</span>
<span class="sd">      marked as non-trainable. `trainable` defaults to `True` unless</span>
<span class="sd">      `synchronization` is set to `ON_READ`.</span>
<span class="sd">    constraint: constraint instance (callable).</span>
<span class="sd">    use_resource: Whether to use `ResourceVariable`.</span>
<span class="sd">    synchronization: Indicates when a distributed a variable will be</span>
<span class="sd">      aggregated. Accepted values are constants defined in the class</span>
<span class="sd">      `tf.VariableSynchronization`. By default the synchronization is set to</span>
<span class="sd">      `AUTO` and the current `DistributionStrategy` chooses</span>
<span class="sd">      when to synchronize. If `synchronization` is set to `ON_READ`,</span>
<span class="sd">      `trainable` must not be set to `True`.</span>
<span class="sd">    aggregation: Indicates how a distributed variable will be aggregated.</span>
<span class="sd">      Accepted values are constants defined in the class</span>
<span class="sd">      `tf.VariableAggregation`.</span>
<span class="sd">    partitioner: (optional) partitioner instance (callable).  If</span>
<span class="sd">      provided, when the requested variable is created it will be split</span>
<span class="sd">      into multiple partitions according to `partitioner`.  In this case,</span>
<span class="sd">      an instance of `PartitionedVariable` is returned.  Available</span>
<span class="sd">      partitioners include `tf.fixed_size_partitioner` and</span>
<span class="sd">      `tf.variable_axis_size_partitioner`.  For more details, see the</span>
<span class="sd">      documentation of `tf.get_variable` and the  &quot;Variable Partitioners</span>
<span class="sd">      and Sharding&quot; section of the API guide.</span>
<span class="sd">  Returns:</span>
<span class="sd">    The created variable.  Usually either a `Variable` or `ResourceVariable`</span>
<span class="sd">    instance.  If `partitioner` is not `None`, a `PartitionedVariable`</span>
<span class="sd">    instance is returned.</span>
<span class="sd">  Raises:</span>
<span class="sd">    RuntimeError: If called with partioned variable regularization and</span>
<span class="sd">      eager execution is enabled.</span>
<span class="sd">    ValueError: When trainable has been set to True with synchronization</span>
<span class="sd">      set as `ON_READ`.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_keras_style</span><span class="p">:</span>
    <span class="k">return</span> <span class="nb">super</span><span class="p">(</span><span class="n">Layer</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="n">add_weight</span><span class="p">(</span>
        <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">,</span>
        <span class="n">shape</span><span class="o">=</span><span class="n">shape</span><span class="p">,</span>
        <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">,</span>
        <span class="n">initializer</span><span class="o">=</span><span class="n">initializer</span><span class="p">,</span>
        <span class="n">regularizer</span><span class="o">=</span><span class="n">regularizer</span><span class="p">,</span>
        <span class="n">trainable</span><span class="o">=</span><span class="n">trainable</span><span class="p">,</span>
        <span class="n">constraint</span><span class="o">=</span><span class="n">constraint</span><span class="p">,</span>
        <span class="n">use_resource</span><span class="o">=</span><span class="n">use_resource</span><span class="p">,</span>
        <span class="n">synchronization</span><span class="o">=</span><span class="n">vs</span><span class="o">.</span><span class="n">VariableSynchronization</span><span class="o">.</span><span class="n">AUTO</span><span class="p">,</span>
        <span class="n">aggregation</span><span class="o">=</span><span class="n">vs</span><span class="o">.</span><span class="n">VariableAggregation</span><span class="o">.</span><span class="n">NONE</span><span class="p">,</span>
        <span class="n">partitioner</span><span class="o">=</span><span class="n">partitioner</span><span class="p">)</span>
  <span class="k">if</span> <span class="n">synchronization</span> <span class="o">==</span> <span class="n">vs</span><span class="o">.</span><span class="n">VariableSynchronization</span><span class="o">.</span><span class="n">ON_READ</span><span class="p">:</span>
    <span class="k">if</span> <span class="n">trainable</span><span class="p">:</span>
      <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
          <span class="s1">&#39;Synchronization value can be set to &#39;</span>
          <span class="s1">&#39;VariableSynchronization.ON_READ only for non-trainable variables. &#39;</span>
          <span class="s1">&#39;You have specified trainable=True and &#39;</span>
          <span class="s1">&#39;synchronization=VariableSynchronization.ON_READ.&#39;</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="c1"># Set trainable to be false when variable is to be synced on read.</span>
      <span class="n">trainable</span> <span class="o">=</span> <span class="bp">False</span>
  <span class="k">elif</span> <span class="n">trainable</span> <span class="ow">is</span> <span class="bp">None</span><span class="p">:</span>
    <span class="n">trainable</span> <span class="o">=</span> <span class="bp">True</span>
  <span class="k">def</span> <span class="nf">_should_add_regularizer</span><span class="p">(</span><span class="n">variable</span><span class="p">,</span> <span class="n">existing_variable_set</span><span class="p">):</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">variable</span><span class="p">,</span> <span class="n">tf_variables</span><span class="o">.</span><span class="n">PartitionedVariable</span><span class="p">):</span>
      <span class="k">for</span> <span class="n">var</span> <span class="ow">in</span> <span class="n">variable</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">var</span> <span class="ow">in</span> <span class="n">existing_variable_set</span><span class="p">:</span>
          <span class="k">return</span> <span class="bp">False</span>
      <span class="k">return</span> <span class="bp">True</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="k">return</span> <span class="n">variable</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">existing_variable_set</span>
  <span class="n">init_graph</span> <span class="o">=</span> <span class="bp">None</span>
  <span class="k">if</span> <span class="ow">not</span> <span class="n">context</span><span class="o">.</span><span class="n">executing_eagerly</span><span class="p">():</span>
    <span class="n">default_graph</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">get_default_graph</span><span class="p">()</span>
    <span class="k">if</span> <span class="n">default_graph</span><span class="o">.</span><span class="n">building_function</span><span class="p">:</span>
      <span class="k">with</span> <span class="n">ops</span><span class="o">.</span><span class="n">init_scope</span><span class="p">():</span>
        <span class="c1"># Retrieve the variables from the graph into which variables</span>
        <span class="c1"># will be lifted; if initialization ops will be lifted into</span>
        <span class="c1"># the eager context, then there is nothing to retrieve, since variable</span>
        <span class="c1"># collections are not supported when eager execution is enabled.</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">context</span><span class="o">.</span><span class="n">executing_eagerly</span><span class="p">():</span>
          <span class="n">init_graph</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">get_default_graph</span><span class="p">()</span>
          <span class="n">existing_variables</span> <span class="o">=</span> <span class="nb">set</span><span class="p">(</span><span class="n">tf_variables</span><span class="o">.</span><span class="n">global_variables</span><span class="p">())</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="c1"># Initialization ops will not be lifted out of the default graph.</span>
      <span class="n">init_graph</span> <span class="o">=</span> <span class="n">default_graph</span>
      <span class="n">existing_variables</span> <span class="o">=</span> <span class="nb">set</span><span class="p">(</span><span class="n">tf_variables</span><span class="o">.</span><span class="n">global_variables</span><span class="p">())</span>
  <span class="k">if</span> <span class="n">dtype</span> <span class="ow">is</span> <span class="bp">None</span><span class="p">:</span>
    <span class="n">dtype</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dtype</span> <span class="ow">or</span> <span class="n">dtypes</span><span class="o">.</span><span class="n">float32</span>
  <span class="bp">self</span><span class="o">.</span><span class="n">_set_scope</span><span class="p">(</span><span class="bp">None</span><span class="p">)</span>
  <span class="n">reuse</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">built</span> <span class="ow">or</span> <span class="bp">self</span><span class="o">.</span><span class="n">_reuse</span>
  <span class="n">prev_len_trainable</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_trainable_weights</span><span class="p">)</span>
  <span class="k">with</span> <span class="n">vs</span><span class="o">.</span><span class="n">variable_scope</span><span class="p">(</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">_scope</span><span class="p">,</span> <span class="n">reuse</span><span class="o">=</span><span class="n">reuse</span><span class="p">,</span> <span class="n">auxiliary_name_scope</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span> <span class="k">as</span> <span class="n">scope</span><span class="p">:</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_current_scope</span> <span class="o">=</span> <span class="n">scope</span>
    <span class="k">with</span> <span class="n">ops</span><span class="o">.</span><span class="n">name_scope</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_name_scope</span><span class="p">()):</span>
      <span class="n">use_resource</span> <span class="o">=</span> <span class="p">(</span><span class="n">use_resource</span> <span class="ow">or</span>
                      <span class="bp">self</span><span class="o">.</span><span class="n">_use_resource_variables</span> <span class="ow">or</span>
                      <span class="n">scope</span><span class="o">.</span><span class="n">use_resource</span><span class="p">)</span>
      <span class="k">if</span> <span class="n">initializer</span> <span class="ow">is</span> <span class="bp">None</span><span class="p">:</span>
        <span class="n">initializer</span> <span class="o">=</span> <span class="n">scope</span><span class="o">.</span><span class="n">initializer</span>
      <span class="n">variable</span> <span class="o">=</span> <span class="nb">super</span><span class="p">(</span><span class="n">Layer</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="n">add_weight</span><span class="p">(</span>
          <span class="n">name</span><span class="p">,</span>
          <span class="n">shape</span><span class="p">,</span>
          <span class="n">dtype</span><span class="o">=</span><span class="n">dtypes</span><span class="o">.</span><span class="n">as_dtype</span><span class="p">(</span><span class="n">dtype</span><span class="p">),</span>
          <span class="n">initializer</span><span class="o">=</span><span class="n">initializer</span><span class="p">,</span>
          <span class="n">trainable</span><span class="o">=</span><span class="n">trainable</span><span class="p">,</span>
          <span class="n">constraint</span><span class="o">=</span><span class="n">constraint</span><span class="p">,</span>
          <span class="n">partitioner</span><span class="o">=</span><span class="n">partitioner</span><span class="p">,</span>
          <span class="n">use_resource</span><span class="o">=</span><span class="n">use_resource</span><span class="p">,</span>
          <span class="n">synchronization</span><span class="o">=</span><span class="n">synchronization</span><span class="p">,</span>
          <span class="n">aggregation</span><span class="o">=</span><span class="n">aggregation</span><span class="p">,</span>
          <span class="n">getter</span><span class="o">=</span><span class="n">vs</span><span class="o">.</span><span class="n">get_variable</span><span class="p">)</span>
      <span class="k">if</span> <span class="n">regularizer</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">context</span><span class="o">.</span><span class="n">executing_eagerly</span><span class="p">()</span> <span class="ow">or</span> <span class="n">_should_add_regularizer</span><span class="p">(</span>
            <span class="n">variable</span><span class="p">,</span> <span class="n">existing_variables</span><span class="p">):</span>
          <span class="bp">self</span><span class="o">.</span><span class="n">_handle_weight_regularization</span><span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="n">variable</span><span class="p">,</span> <span class="n">regularizer</span><span class="p">)</span>
      <span class="k">if</span> <span class="n">init_graph</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span><span class="p">:</span>
        <span class="c1"># Handle edge case where a custom getter has overridden `trainable`.</span>
        <span class="c1"># There is one known occurrence of this, in unit test</span>
        <span class="c1"># testBasicRNNCellNotTrainable in</span>
        <span class="c1"># contrib.rnn.python.kernel_tests.core_rnn_cell_test</span>
        <span class="k">with</span> <span class="n">init_graph</span><span class="o">.</span><span class="n">as_default</span><span class="p">():</span>
          <span class="n">trainable_variables</span> <span class="o">=</span> <span class="n">tf_variables</span><span class="o">.</span><span class="n">trainable_variables</span><span class="p">()</span>
        <span class="k">if</span> <span class="p">(</span><span class="n">trainable</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">trainable</span> <span class="ow">and</span>
            <span class="n">variable</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">trainable_variables</span><span class="p">):</span>
          <span class="c1"># A custom getter / variable scope overrode the trainable flag.</span>
          <span class="n">extra_trainable_vars</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_trainable_weights</span><span class="p">[</span><span class="n">prev_len_trainable</span><span class="p">:]</span>
          <span class="bp">self</span><span class="o">.</span><span class="n">_trainable_weights</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_trainable_weights</span><span class="p">[</span>
              <span class="p">:</span><span class="n">prev_len_trainable</span><span class="p">]</span>
          <span class="bp">self</span><span class="o">.</span><span class="n">_non_trainable_weights</span> <span class="o">+=</span> <span class="n">extra_trainable_vars</span>
  <span class="k">return</span> <span class="n">variable</span>
</pre></div>

  </div>
</div>

  </div>
  
            
  <div class="item">
    <div class="name def" id="latex_ocr.model.components.attention_cell.AttentionCell.apply">
    <p>def <span class="ident">apply</span>(</p><p>self, inputs, *args, **kwargs)</p>
    </div>
    

    
  
    <div class="desc"><p>Apply the layer on a input.</p>
<p>This is an alias of <code>self.__call__</code>.</p>
<p>Arguments:
  inputs: Input tensor(s).
  <em>args: additional positional arguments to be passed to <code>self.call</code>.
  </em>*kwargs: additional keyword arguments to be passed to <code>self.call</code>.</p>
<p>Returns:
  Output tensor(s).</p></div>
  <div class="source_cont">
  <p class="source_link"><a href="javascript:void(0);" onclick="toggle('source-latex_ocr.model.components.attention_cell.AttentionCell.apply', this);">Show source &equiv;</a></p>
  <div id="source-latex_ocr.model.components.attention_cell.AttentionCell.apply" class="source">
    <div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">apply</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Apply the layer on a input.</span>
<span class="sd">  This is an alias of `self.__call__`.</span>
<span class="sd">  Arguments:</span>
<span class="sd">    inputs: Input tensor(s).</span>
<span class="sd">    *args: additional positional arguments to be passed to `self.call`.</span>
<span class="sd">    **kwargs: additional keyword arguments to be passed to `self.call`.</span>
<span class="sd">  Returns:</span>
<span class="sd">    Output tensor(s).</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="fm">__call__</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
</pre></div>

  </div>
</div>

  </div>
  
            
  <div class="item">
    <div class="name def" id="latex_ocr.model.components.attention_cell.AttentionCell.build">
    <p>def <span class="ident">build</span>(</p><p>self, _)</p>
    </div>
    

    
  
    <div class="desc"><p>Creates the variables of the layer (optional, for subclass implementers).</p>
<p>This is a method that implementers of subclasses of <code>Layer</code> or <code>Model</code>
can override if they need a state-creation step in-between
layer instantiation and layer call.</p>
<p>This is typically used to create the weights of <code>Layer</code> subclasses.</p>
<p>Arguments:
  input_shape: Instance of <code>TensorShape</code>, or list of instances of
    <code>TensorShape</code> if the layer expects a list of inputs
    (one instance per input).</p></div>
  <div class="source_cont">
  <p class="source_link"><a href="javascript:void(0);" onclick="toggle('source-latex_ocr.model.components.attention_cell.AttentionCell.build', this);">Show source &equiv;</a></p>
  <div id="source-latex_ocr.model.components.attention_cell.AttentionCell.build" class="source">
    <div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">build</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">_</span><span class="p">):</span>
  <span class="c1"># This tells the parent Layer object that it&#39;s OK to call</span>
  <span class="c1"># self.add_variable() inside the call() method.</span>
  <span class="k">pass</span>
</pre></div>

  </div>
</div>

  </div>
  
            
  <div class="item">
    <div class="name def" id="latex_ocr.model.components.attention_cell.AttentionCell.call">
    <p>def <span class="ident">call</span>(</p><p>self, inputs, **kwargs)</p>
    </div>
    

    
  
    <div class="desc"><p>This is where the layer's logic lives.</p>
<p>Arguments:
    inputs: Input tensor, or list/tuple of input tensors.
    **kwargs: Additional keyword arguments.</p>
<p>Returns:
    A tensor or list/tuple of tensors.</p></div>
  <div class="source_cont">
  <p class="source_link"><a href="javascript:void(0);" onclick="toggle('source-latex_ocr.model.components.attention_cell.AttentionCell.call', this);">Show source &equiv;</a></p>
  <div id="source-latex_ocr.model.components.attention_cell.AttentionCell.call" class="source">
    <div class="codehilite"><pre><span></span><span class="nd">@doc_controls.for_subclass_implementers</span>
<span class="k">def</span> <span class="nf">call</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>  <span class="c1"># pylint: disable=unused-argument</span>
  <span class="sd">&quot;&quot;&quot;This is where the layer&#39;s logic lives.</span>
<span class="sd">  Arguments:</span>
<span class="sd">      inputs: Input tensor, or list/tuple of input tensors.</span>
<span class="sd">      **kwargs: Additional keyword arguments.</span>
<span class="sd">  Returns:</span>
<span class="sd">      A tensor or list/tuple of tensors.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="k">return</span> <span class="n">inputs</span>
</pre></div>

  </div>
</div>

  </div>
  
            
  <div class="item">
    <div class="name def" id="latex_ocr.model.components.attention_cell.AttentionCell.compute_mask">
    <p>def <span class="ident">compute_mask</span>(</p><p>self, inputs, mask=None)</p>
    </div>
    

    
  
    <div class="desc"><p>Computes an output mask tensor.</p>
<p>Arguments:
    inputs: Tensor or list of tensors.
    mask: Tensor or list of tensors.</p>
<p>Returns:
    None or a tensor (or list of tensors,
        one per output tensor of the layer).</p></div>
  <div class="source_cont">
  <p class="source_link"><a href="javascript:void(0);" onclick="toggle('source-latex_ocr.model.components.attention_cell.AttentionCell.compute_mask', this);">Show source &equiv;</a></p>
  <div id="source-latex_ocr.model.components.attention_cell.AttentionCell.compute_mask" class="source">
    <div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">compute_mask</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">mask</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>  <span class="c1"># pylint: disable=unused-argument</span>
  <span class="sd">&quot;&quot;&quot;Computes an output mask tensor.</span>
<span class="sd">  Arguments:</span>
<span class="sd">      inputs: Tensor or list of tensors.</span>
<span class="sd">      mask: Tensor or list of tensors.</span>
<span class="sd">  Returns:</span>
<span class="sd">      None or a tensor (or list of tensors,</span>
<span class="sd">          one per output tensor of the layer).</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">supports_masking</span><span class="p">:</span>
    <span class="k">if</span> <span class="n">mask</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span><span class="p">:</span>
      <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">mask</span><span class="p">,</span> <span class="nb">list</span><span class="p">):</span>
        <span class="k">if</span> <span class="nb">any</span><span class="p">(</span><span class="n">m</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span> <span class="k">for</span> <span class="n">m</span> <span class="ow">in</span> <span class="n">mask</span><span class="p">):</span>
          <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s1">&#39;Layer &#39;</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span> <span class="o">+</span> <span class="s1">&#39; does not support masking, &#39;</span>
                          <span class="s1">&#39;but was passed an input_mask: &#39;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">mask</span><span class="p">))</span>
      <span class="k">else</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s1">&#39;Layer &#39;</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span> <span class="o">+</span> <span class="s1">&#39; does not support masking, &#39;</span>
                        <span class="s1">&#39;but was passed an input_mask: &#39;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">mask</span><span class="p">))</span>
    <span class="c1"># masking not explicitly supported: return None as mask</span>
    <span class="k">return</span> <span class="bp">None</span>
  <span class="c1"># if masking is explicitly supported, by default</span>
  <span class="c1"># carry over the input mask</span>
  <span class="k">return</span> <span class="n">mask</span>
</pre></div>

  </div>
</div>

  </div>
  
            
  <div class="item">
    <div class="name def" id="latex_ocr.model.components.attention_cell.AttentionCell.compute_output_shape">
    <p>def <span class="ident">compute_output_shape</span>(</p><p>self, input_shape)</p>
    </div>
    

    
  
    <div class="desc"><p>Computes the output shape of the layer.</p>
<p>Assumes that the layer will be built
to match that input shape provided.</p>
<p>Arguments:
    input_shape: Shape tuple (tuple of integers)
        or list of shape tuples (one per output tensor of the layer).
        Shape tuples can include None for free dimensions,
        instead of an integer.</p>
<p>Returns:
    An input shape tuple.</p></div>
  <div class="source_cont">
  <p class="source_link"><a href="javascript:void(0);" onclick="toggle('source-latex_ocr.model.components.attention_cell.AttentionCell.compute_output_shape', this);">Show source &equiv;</a></p>
  <div id="source-latex_ocr.model.components.attention_cell.AttentionCell.compute_output_shape" class="source">
    <div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">compute_output_shape</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_shape</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Computes the output shape of the layer.</span>
<span class="sd">  Assumes that the layer will be built</span>
<span class="sd">  to match that input shape provided.</span>
<span class="sd">  Arguments:</span>
<span class="sd">      input_shape: Shape tuple (tuple of integers)</span>
<span class="sd">          or list of shape tuples (one per output tensor of the layer).</span>
<span class="sd">          Shape tuples can include None for free dimensions,</span>
<span class="sd">          instead of an integer.</span>
<span class="sd">  Returns:</span>
<span class="sd">      An input shape tuple.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="k">if</span> <span class="n">context</span><span class="o">.</span><span class="n">executing_eagerly</span><span class="p">():</span>
    <span class="c1"># In this case we build the model first in order to do shape inference.</span>
    <span class="c1"># This is acceptable because the framework only calls</span>
    <span class="c1"># `compute_output_shape` on shape values that the layer would later be</span>
    <span class="c1"># built for. It would however cause issues in case a user attempts to</span>
    <span class="c1"># use `compute_output_shape` manually (these users will have to</span>
    <span class="c1"># implement `compute_output_shape` themselves).</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">build</span><span class="p">(</span><span class="n">input_shape</span><span class="p">)</span>
    <span class="k">with</span> <span class="n">context</span><span class="o">.</span><span class="n">graph_mode</span><span class="p">():</span>
      <span class="n">graph</span> <span class="o">=</span> <span class="n">func_graph</span><span class="o">.</span><span class="n">FuncGraph</span><span class="p">(</span><span class="s1">&#39;graph&#39;</span><span class="p">)</span>
      <span class="k">with</span> <span class="n">graph</span><span class="o">.</span><span class="n">as_default</span><span class="p">():</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">input_shape</span><span class="p">,</span> <span class="nb">list</span><span class="p">):</span>
          <span class="n">inputs</span> <span class="o">=</span> <span class="p">[</span><span class="n">base_layer_utils</span><span class="o">.</span><span class="n">generate_placeholders_from_shape</span><span class="p">(</span><span class="n">shape</span><span class="p">)</span>
                    <span class="k">for</span> <span class="n">shape</span> <span class="ow">in</span> <span class="n">input_shape</span><span class="p">]</span>
        <span class="k">else</span><span class="p">:</span>
          <span class="n">inputs</span> <span class="o">=</span> <span class="n">base_layer_utils</span><span class="o">.</span><span class="n">generate_placeholders_from_shape</span><span class="p">(</span>
              <span class="n">input_shape</span><span class="p">)</span>
        <span class="k">try</span><span class="p">:</span>
          <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_expects_training_arg</span><span class="p">:</span>
            <span class="n">outputs</span> <span class="o">=</span> <span class="bp">self</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">training</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
          <span class="k">else</span><span class="p">:</span>
            <span class="n">outputs</span> <span class="o">=</span> <span class="bp">self</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span>
        <span class="k">except</span> <span class="ne">TypeError</span><span class="p">:</span>
          <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">(</span><span class="s1">&#39;We could not automatically infer &#39;</span>
                                    <span class="s1">&#39;the static shape of the layer</span><span class="se">\&#39;</span><span class="s1">s output.&#39;</span>
                                    <span class="s1">&#39; Please implement the &#39;</span>
                                    <span class="s1">&#39;`compute_output_shape` method on your &#39;</span>
                                    <span class="s1">&#39;layer (</span><span class="si">%s</span><span class="s1">).&#39;</span> <span class="o">%</span> <span class="bp">self</span><span class="o">.</span><span class="vm">__class__</span><span class="o">.</span><span class="vm">__name__</span><span class="p">)</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">outputs</span><span class="p">,</span> <span class="nb">list</span><span class="p">):</span>
      <span class="k">return</span> <span class="p">[</span><span class="n">output</span><span class="o">.</span><span class="n">shape</span> <span class="k">for</span> <span class="n">output</span> <span class="ow">in</span> <span class="n">outputs</span><span class="p">]</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="k">return</span> <span class="n">outputs</span><span class="o">.</span><span class="n">shape</span>
  <span class="k">raise</span> <span class="ne">NotImplementedError</span>
</pre></div>

  </div>
</div>

  </div>
  
            
  <div class="item">
    <div class="name def" id="latex_ocr.model.components.attention_cell.AttentionCell.count_params">
    <p>def <span class="ident">count_params</span>(</p><p>self)</p>
    </div>
    

    
  
    <div class="desc"><p>Count the total number of scalars composing the weights.</p>
<p>Returns:
    An integer count.</p>
<p>Raises:
    ValueError: if the layer isn't yet built
      (in which case its weights aren't yet defined).</p></div>
  <div class="source_cont">
  <p class="source_link"><a href="javascript:void(0);" onclick="toggle('source-latex_ocr.model.components.attention_cell.AttentionCell.count_params', this);">Show source &equiv;</a></p>
  <div id="source-latex_ocr.model.components.attention_cell.AttentionCell.count_params" class="source">
    <div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">count_params</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Count the total number of scalars composing the weights.</span>
<span class="sd">  Returns:</span>
<span class="sd">      An integer count.</span>
<span class="sd">  Raises:</span>
<span class="sd">      ValueError: if the layer isn&#39;t yet built</span>
<span class="sd">        (in which case its weights aren&#39;t yet defined).</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">built</span><span class="p">:</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="vm">__class__</span><span class="o">.</span><span class="vm">__name__</span> <span class="o">==</span> <span class="s1">&#39;Sequential&#39;</span><span class="p">:</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">build</span><span class="p">()</span>  <span class="c1"># pylint: disable=no-value-for-parameter</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s1">&#39;You tried to call `count_params` on &#39;</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span> <span class="o">+</span>
                       <span class="s1">&#39;, but the layer isn</span><span class="se">\&#39;</span><span class="s1">t built. &#39;</span>
                       <span class="s1">&#39;You can build it manually via: `&#39;</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span> <span class="o">+</span>
                       <span class="s1">&#39;.build(batch_input_shape)`.&#39;</span><span class="p">)</span>
  <span class="k">return</span> <span class="nb">int</span><span class="p">(</span><span class="nb">sum</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">prod</span><span class="p">(</span><span class="n">w</span><span class="o">.</span><span class="n">shape</span><span class="o">.</span><span class="n">as_list</span><span class="p">())</span> <span class="k">for</span> <span class="n">w</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">weights</span><span class="p">))</span>
</pre></div>

  </div>
</div>

  </div>
  
            
  <div class="item">
    <div class="name def" id="latex_ocr.model.components.attention_cell.AttentionCell.get_config">
    <p>def <span class="ident">get_config</span>(</p><p>self)</p>
    </div>
    

    
  
    <div class="desc"><p>Returns the config of the layer.</p>
<p>A layer config is a Python dictionary (serializable)
containing the configuration of a layer.
The same layer can be reinstantiated later
(without its trained weights) from this configuration.</p>
<p>The config of a layer does not include connectivity
information, nor the layer class name. These are handled
by <code>Network</code> (one layer of abstraction above).</p>
<p>Returns:
    Python dictionary.</p></div>
  <div class="source_cont">
  <p class="source_link"><a href="javascript:void(0);" onclick="toggle('source-latex_ocr.model.components.attention_cell.AttentionCell.get_config', this);">Show source &equiv;</a></p>
  <div id="source-latex_ocr.model.components.attention_cell.AttentionCell.get_config" class="source">
    <div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">get_config</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Returns the config of the layer.</span>
<span class="sd">  A layer config is a Python dictionary (serializable)</span>
<span class="sd">  containing the configuration of a layer.</span>
<span class="sd">  The same layer can be reinstantiated later</span>
<span class="sd">  (without its trained weights) from this configuration.</span>
<span class="sd">  The config of a layer does not include connectivity</span>
<span class="sd">  information, nor the layer class name. These are handled</span>
<span class="sd">  by `Network` (one layer of abstraction above).</span>
<span class="sd">  Returns:</span>
<span class="sd">      Python dictionary.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="n">config</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;name&#39;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">,</span> <span class="s1">&#39;trainable&#39;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">trainable</span><span class="p">}</span>
  <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s1">&#39;_batch_input_shape&#39;</span><span class="p">):</span>
    <span class="n">config</span><span class="p">[</span><span class="s1">&#39;batch_input_shape&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_batch_input_shape</span>
  <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s1">&#39;dtype&#39;</span><span class="p">):</span>
    <span class="n">config</span><span class="p">[</span><span class="s1">&#39;dtype&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dtype</span>
  <span class="k">return</span> <span class="n">config</span>
</pre></div>

  </div>
</div>

  </div>
  
            
  <div class="item">
    <div class="name def" id="latex_ocr.model.components.attention_cell.AttentionCell.get_initial_state">
    <p>def <span class="ident">get_initial_state</span>(</p><p>self, inputs=None, batch_size=None, dtype=None)</p>
    </div>
    

    
  
  <div class="source_cont">
  <p class="source_link"><a href="javascript:void(0);" onclick="toggle('source-latex_ocr.model.components.attention_cell.AttentionCell.get_initial_state', this);">Show source &equiv;</a></p>
  <div id="source-latex_ocr.model.components.attention_cell.AttentionCell.get_initial_state" class="source">
    <div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">get_initial_state</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputs</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
  <span class="k">if</span> <span class="n">inputs</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span><span class="p">:</span>
    <span class="c1"># Validate the given batch_size and dtype against inputs if provided.</span>
    <span class="n">inputs</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">convert_to_tensor</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;inputs&quot;</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">batch_size</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span><span class="p">:</span>
      <span class="k">if</span> <span class="n">tensor_util</span><span class="o">.</span><span class="n">is_tensor</span><span class="p">(</span><span class="n">batch_size</span><span class="p">):</span>
        <span class="n">static_batch_size</span> <span class="o">=</span> <span class="n">tensor_util</span><span class="o">.</span><span class="n">constant_value</span><span class="p">(</span>
            <span class="n">batch_size</span><span class="p">,</span> <span class="n">partial</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
      <span class="k">else</span><span class="p">:</span>
        <span class="n">static_batch_size</span> <span class="o">=</span> <span class="n">batch_size</span>
      <span class="k">if</span> <span class="n">inputs</span><span class="o">.</span><span class="n">shape</span><span class="o">.</span><span class="n">dims</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">value</span> <span class="o">!=</span> <span class="n">static_batch_size</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
            <span class="s2">&quot;batch size from input tensor is different from the &quot;</span>
            <span class="s2">&quot;input param. Input tensor batch: {}, batch_size: {}&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span>
                <span class="n">inputs</span><span class="o">.</span><span class="n">shape</span><span class="o">.</span><span class="n">dims</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">value</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">))</span>
    <span class="k">if</span> <span class="n">dtype</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span> <span class="ow">and</span> <span class="n">inputs</span><span class="o">.</span><span class="n">dtype</span> <span class="o">!=</span> <span class="n">dtype</span><span class="p">:</span>
      <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
          <span class="s2">&quot;dtype from input tensor is different from the &quot;</span>
          <span class="s2">&quot;input param. Input tensor dtype: {}, dtype: {}&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span>
              <span class="n">inputs</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span> <span class="n">dtype</span><span class="p">))</span>
    <span class="n">batch_size</span> <span class="o">=</span> <span class="n">inputs</span><span class="o">.</span><span class="n">shape</span><span class="o">.</span><span class="n">dims</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">value</span> <span class="ow">or</span> <span class="n">array_ops</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">inputs</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">dtype</span> <span class="o">=</span> <span class="n">inputs</span><span class="o">.</span><span class="n">dtype</span>
  <span class="k">if</span> <span class="bp">None</span> <span class="ow">in</span> <span class="p">[</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">dtype</span><span class="p">]:</span>
    <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
        <span class="s2">&quot;batch_size and dtype cannot be None while constructing initial &quot;</span>
        <span class="s2">&quot;state: batch_size={}, dtype={}&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">dtype</span><span class="p">))</span>
  <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">zero_state</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">dtype</span><span class="p">)</span>
</pre></div>

  </div>
</div>

  </div>
  
            
  <div class="item">
    <div class="name def" id="latex_ocr.model.components.attention_cell.AttentionCell.get_input_at">
    <p>def <span class="ident">get_input_at</span>(</p><p>self, node_index)</p>
    </div>
    

    
  
    <div class="desc"><p>Retrieves the input tensor(s) of a layer at a given node.</p>
<p>Arguments:
    node_index: Integer, index of the node
        from which to retrieve the attribute.
        E.g. <code>node_index=0</code> will correspond to the
        first time the layer was called.</p>
<p>Returns:
    A tensor (or list of tensors if the layer has multiple inputs).</p>
<p>Raises:
  RuntimeError: If called in Eager mode.</p></div>
  <div class="source_cont">
  <p class="source_link"><a href="javascript:void(0);" onclick="toggle('source-latex_ocr.model.components.attention_cell.AttentionCell.get_input_at', this);">Show source &equiv;</a></p>
  <div id="source-latex_ocr.model.components.attention_cell.AttentionCell.get_input_at" class="source">
    <div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">get_input_at</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">node_index</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Retrieves the input tensor(s) of a layer at a given node.</span>
<span class="sd">  Arguments:</span>
<span class="sd">      node_index: Integer, index of the node</span>
<span class="sd">          from which to retrieve the attribute.</span>
<span class="sd">          E.g. `node_index=0` will correspond to the</span>
<span class="sd">          first time the layer was called.</span>
<span class="sd">  Returns:</span>
<span class="sd">      A tensor (or list of tensors if the layer has multiple inputs).</span>
<span class="sd">  Raises:</span>
<span class="sd">    RuntimeError: If called in Eager mode.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_get_node_attribute_at_index</span><span class="p">(</span><span class="n">node_index</span><span class="p">,</span> <span class="s1">&#39;input_tensors&#39;</span><span class="p">,</span>
                                           <span class="s1">&#39;input&#39;</span><span class="p">)</span>
</pre></div>

  </div>
</div>

  </div>
  
            
  <div class="item">
    <div class="name def" id="latex_ocr.model.components.attention_cell.AttentionCell.get_input_mask_at">
    <p>def <span class="ident">get_input_mask_at</span>(</p><p>self, node_index)</p>
    </div>
    

    
  
    <div class="desc"><p>Retrieves the input mask tensor(s) of a layer at a given node.</p>
<p>Arguments:
    node_index: Integer, index of the node
        from which to retrieve the attribute.
        E.g. <code>node_index=0</code> will correspond to the
        first time the layer was called.</p>
<p>Returns:
    A mask tensor
    (or list of tensors if the layer has multiple inputs).</p></div>
  <div class="source_cont">
  <p class="source_link"><a href="javascript:void(0);" onclick="toggle('source-latex_ocr.model.components.attention_cell.AttentionCell.get_input_mask_at', this);">Show source &equiv;</a></p>
  <div id="source-latex_ocr.model.components.attention_cell.AttentionCell.get_input_mask_at" class="source">
    <div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">get_input_mask_at</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">node_index</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Retrieves the input mask tensor(s) of a layer at a given node.</span>
<span class="sd">  Arguments:</span>
<span class="sd">      node_index: Integer, index of the node</span>
<span class="sd">          from which to retrieve the attribute.</span>
<span class="sd">          E.g. `node_index=0` will correspond to the</span>
<span class="sd">          first time the layer was called.</span>
<span class="sd">  Returns:</span>
<span class="sd">      A mask tensor</span>
<span class="sd">      (or list of tensors if the layer has multiple inputs).</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="n">inputs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_input_at</span><span class="p">(</span><span class="n">node_index</span><span class="p">)</span>
  <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="nb">list</span><span class="p">):</span>
    <span class="k">return</span> <span class="p">[</span><span class="nb">getattr</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="s1">&#39;_keras_mask&#39;</span><span class="p">,</span> <span class="bp">None</span><span class="p">)</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">inputs</span><span class="p">]</span>
  <span class="k">else</span><span class="p">:</span>
    <span class="k">return</span> <span class="nb">getattr</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="s1">&#39;_keras_mask&#39;</span><span class="p">,</span> <span class="bp">None</span><span class="p">)</span>
</pre></div>

  </div>
</div>

  </div>
  
            
  <div class="item">
    <div class="name def" id="latex_ocr.model.components.attention_cell.AttentionCell.get_input_shape_at">
    <p>def <span class="ident">get_input_shape_at</span>(</p><p>self, node_index)</p>
    </div>
    

    
  
    <div class="desc"><p>Retrieves the input shape(s) of a layer at a given node.</p>
<p>Arguments:
    node_index: Integer, index of the node
        from which to retrieve the attribute.
        E.g. <code>node_index=0</code> will correspond to the
        first time the layer was called.</p>
<p>Returns:
    A shape tuple
    (or list of shape tuples if the layer has multiple inputs).</p>
<p>Raises:
  RuntimeError: If called in Eager mode.</p></div>
  <div class="source_cont">
  <p class="source_link"><a href="javascript:void(0);" onclick="toggle('source-latex_ocr.model.components.attention_cell.AttentionCell.get_input_shape_at', this);">Show source &equiv;</a></p>
  <div id="source-latex_ocr.model.components.attention_cell.AttentionCell.get_input_shape_at" class="source">
    <div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">get_input_shape_at</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">node_index</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Retrieves the input shape(s) of a layer at a given node.</span>
<span class="sd">  Arguments:</span>
<span class="sd">      node_index: Integer, index of the node</span>
<span class="sd">          from which to retrieve the attribute.</span>
<span class="sd">          E.g. `node_index=0` will correspond to the</span>
<span class="sd">          first time the layer was called.</span>
<span class="sd">  Returns:</span>
<span class="sd">      A shape tuple</span>
<span class="sd">      (or list of shape tuples if the layer has multiple inputs).</span>
<span class="sd">  Raises:</span>
<span class="sd">    RuntimeError: If called in Eager mode.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_get_node_attribute_at_index</span><span class="p">(</span><span class="n">node_index</span><span class="p">,</span> <span class="s1">&#39;input_shapes&#39;</span><span class="p">,</span>
                                           <span class="s1">&#39;input shape&#39;</span><span class="p">)</span>
</pre></div>

  </div>
</div>

  </div>
  
            
  <div class="item">
    <div class="name def" id="latex_ocr.model.components.attention_cell.AttentionCell.get_losses_for">
    <p>def <span class="ident">get_losses_for</span>(</p><p>self, inputs)</p>
    </div>
    

    
  
    <div class="desc"><p>Retrieves losses relevant to a specific set of inputs.</p>
<p>Arguments:
  inputs: Input tensor or list/tuple of input tensors.</p>
<p>Returns:
  List of loss tensors of the layer that depend on <code>inputs</code>.</p>
<p>Raises:
  RuntimeError: If called in Eager mode.</p></div>
  <div class="source_cont">
  <p class="source_link"><a href="javascript:void(0);" onclick="toggle('source-latex_ocr.model.components.attention_cell.AttentionCell.get_losses_for', this);">Show source &equiv;</a></p>
  <div id="source-latex_ocr.model.components.attention_cell.AttentionCell.get_losses_for" class="source">
    <div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">get_losses_for</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputs</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Retrieves losses relevant to a specific set of inputs.</span>
<span class="sd">  Arguments:</span>
<span class="sd">    inputs: Input tensor or list/tuple of input tensors.</span>
<span class="sd">  Returns:</span>
<span class="sd">    List of loss tensors of the layer that depend on `inputs`.</span>
<span class="sd">  Raises:</span>
<span class="sd">    RuntimeError: If called in Eager mode.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="k">if</span> <span class="n">inputs</span> <span class="ow">is</span> <span class="bp">None</span><span class="p">:</span>
    <span class="c1"># Requesting unconditional losses.</span>
    <span class="k">return</span> <span class="p">[</span><span class="n">x</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">losses</span> <span class="k">if</span> <span class="n">x</span><span class="o">.</span><span class="n">_unconditional_loss</span><span class="p">]</span>  <span class="c1"># pylint: disable=protected-access</span>
  <span class="c1"># Requesting input-conditional losses.</span>
  <span class="n">inputs</span> <span class="o">=</span> <span class="n">nest</span><span class="o">.</span><span class="n">flatten</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span>
  <span class="c1"># Retrieve the set of tensors in the TF graph that depend on `inputs`.</span>
  <span class="c1"># The losses we want to return will be part of this set.</span>
  <span class="c1"># To avoid unnecessary work, we stop the search in case all of</span>
  <span class="c1"># `self.losses` have been retrieved.</span>
  <span class="n">reachable</span> <span class="o">=</span> <span class="n">tf_utils</span><span class="o">.</span><span class="n">get_reachable_from_inputs</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">losses</span><span class="p">)</span>
  <span class="n">losses</span> <span class="o">=</span> <span class="p">[]</span>
  <span class="k">for</span> <span class="n">loss</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">losses</span><span class="p">:</span>
    <span class="k">if</span> <span class="n">loss</span> <span class="ow">in</span> <span class="n">reachable</span><span class="p">:</span>
      <span class="n">losses</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">loss</span><span class="p">)</span>
  <span class="k">return</span> <span class="n">losses</span>
</pre></div>

  </div>
</div>

  </div>
  
            
  <div class="item">
    <div class="name def" id="latex_ocr.model.components.attention_cell.AttentionCell.get_output_at">
    <p>def <span class="ident">get_output_at</span>(</p><p>self, node_index)</p>
    </div>
    

    
  
    <div class="desc"><p>Retrieves the output tensor(s) of a layer at a given node.</p>
<p>Arguments:
    node_index: Integer, index of the node
        from which to retrieve the attribute.
        E.g. <code>node_index=0</code> will correspond to the
        first time the layer was called.</p>
<p>Returns:
    A tensor (or list of tensors if the layer has multiple outputs).</p>
<p>Raises:
  RuntimeError: If called in Eager mode.</p></div>
  <div class="source_cont">
  <p class="source_link"><a href="javascript:void(0);" onclick="toggle('source-latex_ocr.model.components.attention_cell.AttentionCell.get_output_at', this);">Show source &equiv;</a></p>
  <div id="source-latex_ocr.model.components.attention_cell.AttentionCell.get_output_at" class="source">
    <div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">get_output_at</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">node_index</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Retrieves the output tensor(s) of a layer at a given node.</span>
<span class="sd">  Arguments:</span>
<span class="sd">      node_index: Integer, index of the node</span>
<span class="sd">          from which to retrieve the attribute.</span>
<span class="sd">          E.g. `node_index=0` will correspond to the</span>
<span class="sd">          first time the layer was called.</span>
<span class="sd">  Returns:</span>
<span class="sd">      A tensor (or list of tensors if the layer has multiple outputs).</span>
<span class="sd">  Raises:</span>
<span class="sd">    RuntimeError: If called in Eager mode.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_get_node_attribute_at_index</span><span class="p">(</span><span class="n">node_index</span><span class="p">,</span> <span class="s1">&#39;output_tensors&#39;</span><span class="p">,</span>
                                           <span class="s1">&#39;output&#39;</span><span class="p">)</span>
</pre></div>

  </div>
</div>

  </div>
  
            
  <div class="item">
    <div class="name def" id="latex_ocr.model.components.attention_cell.AttentionCell.get_output_mask_at">
    <p>def <span class="ident">get_output_mask_at</span>(</p><p>self, node_index)</p>
    </div>
    

    
  
    <div class="desc"><p>Retrieves the output mask tensor(s) of a layer at a given node.</p>
<p>Arguments:
    node_index: Integer, index of the node
        from which to retrieve the attribute.
        E.g. <code>node_index=0</code> will correspond to the
        first time the layer was called.</p>
<p>Returns:
    A mask tensor
    (or list of tensors if the layer has multiple outputs).</p></div>
  <div class="source_cont">
  <p class="source_link"><a href="javascript:void(0);" onclick="toggle('source-latex_ocr.model.components.attention_cell.AttentionCell.get_output_mask_at', this);">Show source &equiv;</a></p>
  <div id="source-latex_ocr.model.components.attention_cell.AttentionCell.get_output_mask_at" class="source">
    <div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">get_output_mask_at</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">node_index</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Retrieves the output mask tensor(s) of a layer at a given node.</span>
<span class="sd">  Arguments:</span>
<span class="sd">      node_index: Integer, index of the node</span>
<span class="sd">          from which to retrieve the attribute.</span>
<span class="sd">          E.g. `node_index=0` will correspond to the</span>
<span class="sd">          first time the layer was called.</span>
<span class="sd">  Returns:</span>
<span class="sd">      A mask tensor</span>
<span class="sd">      (or list of tensors if the layer has multiple outputs).</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="n">output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_output_at</span><span class="p">(</span><span class="n">node_index</span><span class="p">)</span>
  <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="nb">list</span><span class="p">):</span>
    <span class="k">return</span> <span class="p">[</span><span class="nb">getattr</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="s1">&#39;_keras_mask&#39;</span><span class="p">,</span> <span class="bp">None</span><span class="p">)</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">output</span><span class="p">]</span>
  <span class="k">else</span><span class="p">:</span>
    <span class="k">return</span> <span class="nb">getattr</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="s1">&#39;_keras_mask&#39;</span><span class="p">,</span> <span class="bp">None</span><span class="p">)</span>
</pre></div>

  </div>
</div>

  </div>
  
            
  <div class="item">
    <div class="name def" id="latex_ocr.model.components.attention_cell.AttentionCell.get_output_shape_at">
    <p>def <span class="ident">get_output_shape_at</span>(</p><p>self, node_index)</p>
    </div>
    

    
  
    <div class="desc"><p>Retrieves the output shape(s) of a layer at a given node.</p>
<p>Arguments:
    node_index: Integer, index of the node
        from which to retrieve the attribute.
        E.g. <code>node_index=0</code> will correspond to the
        first time the layer was called.</p>
<p>Returns:
    A shape tuple
    (or list of shape tuples if the layer has multiple outputs).</p>
<p>Raises:
  RuntimeError: If called in Eager mode.</p></div>
  <div class="source_cont">
  <p class="source_link"><a href="javascript:void(0);" onclick="toggle('source-latex_ocr.model.components.attention_cell.AttentionCell.get_output_shape_at', this);">Show source &equiv;</a></p>
  <div id="source-latex_ocr.model.components.attention_cell.AttentionCell.get_output_shape_at" class="source">
    <div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">get_output_shape_at</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">node_index</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Retrieves the output shape(s) of a layer at a given node.</span>
<span class="sd">  Arguments:</span>
<span class="sd">      node_index: Integer, index of the node</span>
<span class="sd">          from which to retrieve the attribute.</span>
<span class="sd">          E.g. `node_index=0` will correspond to the</span>
<span class="sd">          first time the layer was called.</span>
<span class="sd">  Returns:</span>
<span class="sd">      A shape tuple</span>
<span class="sd">      (or list of shape tuples if the layer has multiple outputs).</span>
<span class="sd">  Raises:</span>
<span class="sd">    RuntimeError: If called in Eager mode.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_get_node_attribute_at_index</span><span class="p">(</span><span class="n">node_index</span><span class="p">,</span> <span class="s1">&#39;output_shapes&#39;</span><span class="p">,</span>
                                           <span class="s1">&#39;output shape&#39;</span><span class="p">)</span>
</pre></div>

  </div>
</div>

  </div>
  
            
  <div class="item">
    <div class="name def" id="latex_ocr.model.components.attention_cell.AttentionCell.get_updates_for">
    <p>def <span class="ident">get_updates_for</span>(</p><p>self, inputs)</p>
    </div>
    

    
  
    <div class="desc"><p>Retrieves updates relevant to a specific set of inputs.</p>
<p>Arguments:
  inputs: Input tensor or list/tuple of input tensors.</p>
<p>Returns:
  List of update ops of the layer that depend on <code>inputs</code>.</p>
<p>Raises:
  RuntimeError: If called in Eager mode.</p></div>
  <div class="source_cont">
  <p class="source_link"><a href="javascript:void(0);" onclick="toggle('source-latex_ocr.model.components.attention_cell.AttentionCell.get_updates_for', this);">Show source &equiv;</a></p>
  <div id="source-latex_ocr.model.components.attention_cell.AttentionCell.get_updates_for" class="source">
    <div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">get_updates_for</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputs</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Retrieves updates relevant to a specific set of inputs.</span>
<span class="sd">  Arguments:</span>
<span class="sd">    inputs: Input tensor or list/tuple of input tensors.</span>
<span class="sd">  Returns:</span>
<span class="sd">    List of update ops of the layer that depend on `inputs`.</span>
<span class="sd">  Raises:</span>
<span class="sd">    RuntimeError: If called in Eager mode.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="c1"># Updates disabled if layer is not trainable and not explicitly stateful.</span>
  <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">trainable</span> <span class="ow">and</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">stateful</span><span class="p">:</span>
    <span class="k">return</span> <span class="p">[]</span>
  <span class="k">if</span> <span class="n">inputs</span> <span class="ow">is</span> <span class="bp">None</span><span class="p">:</span>
    <span class="c1"># Requesting unconditional updates.</span>
    <span class="k">return</span> <span class="p">[</span><span class="n">x</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">updates</span> <span class="k">if</span> <span class="n">x</span><span class="o">.</span><span class="n">_unconditional_update</span><span class="p">]</span>  <span class="c1"># pylint: disable=protected-access</span>
  <span class="c1"># Requesting input-conditional updates.</span>
  <span class="n">inputs</span> <span class="o">=</span> <span class="n">nest</span><span class="o">.</span><span class="n">flatten</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span>
  <span class="n">reachable</span> <span class="o">=</span> <span class="n">tf_utils</span><span class="o">.</span><span class="n">get_reachable_from_inputs</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">updates</span><span class="p">)</span>
  <span class="n">updates</span> <span class="o">=</span> <span class="p">[]</span>
  <span class="k">for</span> <span class="n">update</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">updates</span><span class="p">:</span>
    <span class="k">if</span> <span class="n">update</span> <span class="ow">in</span> <span class="n">reachable</span><span class="p">:</span>
      <span class="n">updates</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">update</span><span class="p">)</span>
  <span class="k">return</span> <span class="n">updates</span>
</pre></div>

  </div>
</div>

  </div>
  
            
  <div class="item">
    <div class="name def" id="latex_ocr.model.components.attention_cell.AttentionCell.get_weights">
    <p>def <span class="ident">get_weights</span>(</p><p>self)</p>
    </div>
    

    
  
    <div class="desc"><p>Returns the current weights of the layer.</p>
<p>Returns:
    Weights values as a list of numpy arrays.</p></div>
  <div class="source_cont">
  <p class="source_link"><a href="javascript:void(0);" onclick="toggle('source-latex_ocr.model.components.attention_cell.AttentionCell.get_weights', this);">Show source &equiv;</a></p>
  <div id="source-latex_ocr.model.components.attention_cell.AttentionCell.get_weights" class="source">
    <div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">get_weights</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Returns the current weights of the layer.</span>
<span class="sd">  Returns:</span>
<span class="sd">      Weights values as a list of numpy arrays.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="n">params</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">weights</span>
  <span class="k">return</span> <span class="n">backend</span><span class="o">.</span><span class="n">batch_get_value</span><span class="p">(</span><span class="n">params</span><span class="p">)</span>
</pre></div>

  </div>
</div>

  </div>
  
            
  <div class="item">
    <div class="name def" id="latex_ocr.model.components.attention_cell.AttentionCell.initial_state">
    <p>def <span class="ident">initial_state</span>(</p><p>self)</p>
    </div>
    

    
  
    <div class="desc"><p>Returns initial state for the lstm</p></div>
  <div class="source_cont">
  <p class="source_link"><a href="javascript:void(0);" onclick="toggle('source-latex_ocr.model.components.attention_cell.AttentionCell.initial_state', this);">Show source &equiv;</a></p>
  <div id="source-latex_ocr.model.components.attention_cell.AttentionCell.initial_state" class="source">
    <div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">initial_state</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Returns initial state for the lstm&quot;&quot;&quot;</span>
    <span class="n">initial_cell_state</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_attention_mechanism</span><span class="o">.</span><span class="n">initial_cell_state</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_cell</span><span class="p">)</span>
    <span class="n">initial_o</span>          <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_attention_mechanism</span><span class="o">.</span><span class="n">initial_state</span><span class="p">(</span><span class="s2">&quot;o&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_dim_o</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">AttentionState</span><span class="p">(</span><span class="n">initial_cell_state</span><span class="p">,</span> <span class="n">initial_o</span><span class="p">)</span>
</pre></div>

  </div>
</div>

  </div>
  
            
  <div class="item">
    <div class="name def" id="latex_ocr.model.components.attention_cell.AttentionCell.set_weights">
    <p>def <span class="ident">set_weights</span>(</p><p>self, weights)</p>
    </div>
    

    
  
    <div class="desc"><p>Sets the weights of the layer, from Numpy arrays.</p>
<p>Arguments:
    weights: a list of Numpy arrays. The number
        of arrays and their shape must match
        number of the dimensions of the weights
        of the layer (i.e. it should match the
        output of <code>get_weights</code>).</p>
<p>Raises:
    ValueError: If the provided weights list does not match the
        layer's specifications.</p></div>
  <div class="source_cont">
  <p class="source_link"><a href="javascript:void(0);" onclick="toggle('source-latex_ocr.model.components.attention_cell.AttentionCell.set_weights', this);">Show source &equiv;</a></p>
  <div id="source-latex_ocr.model.components.attention_cell.AttentionCell.set_weights" class="source">
    <div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">set_weights</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">weights</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Sets the weights of the layer, from Numpy arrays.</span>
<span class="sd">  Arguments:</span>
<span class="sd">      weights: a list of Numpy arrays. The number</span>
<span class="sd">          of arrays and their shape must match</span>
<span class="sd">          number of the dimensions of the weights</span>
<span class="sd">          of the layer (i.e. it should match the</span>
<span class="sd">          output of `get_weights`).</span>
<span class="sd">  Raises:</span>
<span class="sd">      ValueError: If the provided weights list does not match the</span>
<span class="sd">          layer&#39;s specifications.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="n">params</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">weights</span>
  <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">params</span><span class="p">)</span> <span class="o">!=</span> <span class="nb">len</span><span class="p">(</span><span class="n">weights</span><span class="p">):</span>
    <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s1">&#39;You called `set_weights(weights)` on layer &quot;&#39;</span> <span class="o">+</span>
                     <span class="bp">self</span><span class="o">.</span><span class="n">name</span> <span class="o">+</span> <span class="s1">&#39;&quot; with a  weight list of length &#39;</span> <span class="o">+</span>
                     <span class="nb">str</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">weights</span><span class="p">))</span> <span class="o">+</span> <span class="s1">&#39;, but the layer was expecting &#39;</span> <span class="o">+</span>
                     <span class="nb">str</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">params</span><span class="p">))</span> <span class="o">+</span> <span class="s1">&#39; weights. Provided weights: &#39;</span> <span class="o">+</span>
                     <span class="nb">str</span><span class="p">(</span><span class="n">weights</span><span class="p">)[:</span><span class="mi">50</span><span class="p">]</span> <span class="o">+</span> <span class="s1">&#39;...&#39;</span><span class="p">)</span>
  <span class="k">if</span> <span class="ow">not</span> <span class="n">params</span><span class="p">:</span>
    <span class="k">return</span>
  <span class="n">weight_value_tuples</span> <span class="o">=</span> <span class="p">[]</span>
  <span class="n">param_values</span> <span class="o">=</span> <span class="n">backend</span><span class="o">.</span><span class="n">batch_get_value</span><span class="p">(</span><span class="n">params</span><span class="p">)</span>
  <span class="k">for</span> <span class="n">pv</span><span class="p">,</span> <span class="n">p</span><span class="p">,</span> <span class="n">w</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">param_values</span><span class="p">,</span> <span class="n">params</span><span class="p">,</span> <span class="n">weights</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">pv</span><span class="o">.</span><span class="n">shape</span> <span class="o">!=</span> <span class="n">w</span><span class="o">.</span><span class="n">shape</span><span class="p">:</span>
      <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s1">&#39;Layer weight shape &#39;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">pv</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span> <span class="o">+</span>
                       <span class="s1">&#39; not compatible with &#39;</span>
                       <span class="s1">&#39;provided weight shape &#39;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">w</span><span class="o">.</span><span class="n">shape</span><span class="p">))</span>
    <span class="n">weight_value_tuples</span><span class="o">.</span><span class="n">append</span><span class="p">((</span><span class="n">p</span><span class="p">,</span> <span class="n">w</span><span class="p">))</span>
  <span class="n">backend</span><span class="o">.</span><span class="n">batch_set_value</span><span class="p">(</span><span class="n">weight_value_tuples</span><span class="p">)</span>
</pre></div>

  </div>
</div>

  </div>
  
            
  <div class="item">
    <div class="name def" id="latex_ocr.model.components.attention_cell.AttentionCell.step">
    <p>def <span class="ident">step</span>(</p><p>self, embedding, attn_cell_state)</p>
    </div>
    

    
  
    <div class="desc"><p>Args:
embedding: shape = (batch_size, dim_embeddings) embeddings
    from previous time step
attn_cell_state: (AttentionState) state from previous time step</p></div>
  <div class="source_cont">
  <p class="source_link"><a href="javascript:void(0);" onclick="toggle('source-latex_ocr.model.components.attention_cell.AttentionCell.step', this);">Show source &equiv;</a></p>
  <div id="source-latex_ocr.model.components.attention_cell.AttentionCell.step" class="source">
    <div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">embedding</span><span class="p">,</span> <span class="n">attn_cell_state</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Args:</span>
<span class="sd">        embedding: shape = (batch_size, dim_embeddings) embeddings</span>
<span class="sd">            from previous time step</span>
<span class="sd">        attn_cell_state: (AttentionState) state from previous time step</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">prev_cell_state</span><span class="p">,</span> <span class="n">o</span> <span class="o">=</span> <span class="n">attn_cell_state</span>
    <span class="n">scope</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">get_variable_scope</span><span class="p">()</span>
    <span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">variable_scope</span><span class="p">(</span><span class="n">scope</span><span class="p">):</span>
        <span class="c1"># compute new h</span>
        <span class="n">x</span>                     <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">concat</span><span class="p">([</span><span class="n">embedding</span><span class="p">,</span> <span class="n">o</span><span class="p">],</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">new_h</span><span class="p">,</span> <span class="n">new_cell_state</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_cell</span><span class="o">.</span><span class="fm">__call__</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">prev_cell_state</span><span class="p">)</span>
        <span class="n">new_h</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">dropout</span><span class="p">(</span><span class="n">new_h</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_dropout</span><span class="p">)</span>
        <span class="c1"># compute attention</span>
        <span class="n">c</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_attention_mechanism</span><span class="o">.</span><span class="n">context</span><span class="p">(</span><span class="n">new_h</span><span class="p">)</span>
        <span class="c1"># compute o</span>
        <span class="n">o_W_c</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">get_variable</span><span class="p">(</span><span class="s2">&quot;o_W_c&quot;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span>
                <span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_n_channels</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_dim_o</span><span class="p">))</span>
        <span class="n">o_W_h</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">get_variable</span><span class="p">(</span><span class="s2">&quot;o_W_h&quot;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span>
                <span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_num_units</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_dim_o</span><span class="p">))</span>
        <span class="n">new_o</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">tanh</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">new_h</span><span class="p">,</span> <span class="n">o_W_h</span><span class="p">)</span> <span class="o">+</span> <span class="n">tf</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">c</span><span class="p">,</span> <span class="n">o_W_c</span><span class="p">))</span>
        <span class="n">new_o</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">dropout</span><span class="p">(</span><span class="n">new_o</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_dropout</span><span class="p">)</span>
        <span class="n">y_W_o</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">get_variable</span><span class="p">(</span><span class="s2">&quot;y_W_o&quot;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span>
                <span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_dim_o</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_num_proj</span><span class="p">))</span>
        <span class="n">logits</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">new_o</span><span class="p">,</span> <span class="n">y_W_o</span><span class="p">)</span>
        <span class="c1"># new Attn cell state</span>
        <span class="n">new_state</span> <span class="o">=</span> <span class="n">AttentionState</span><span class="p">(</span><span class="n">new_cell_state</span><span class="p">,</span> <span class="n">new_o</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">logits</span><span class="p">,</span> <span class="n">new_state</span>
</pre></div>

  </div>
</div>

  </div>
  
            
  <div class="item">
    <div class="name def" id="latex_ocr.model.components.attention_cell.AttentionCell.zero_state">
    <p>def <span class="ident">zero_state</span>(</p><p>self, batch_size, dtype)</p>
    </div>
    

    
  
    <div class="desc"><p>Return zero-filled state tensor(s).</p>
<p>Args:
  batch_size: int, float, or unit Tensor representing the batch size.
  dtype: the data type to use for the state.</p>
<p>Returns:
  If <code>state_size</code> is an int or TensorShape, then the return value is a
  <code>N-D</code> tensor of shape <code>[batch_size, state_size]</code> filled with zeros.</p>
<p>If <code>state_size</code> is a nested list or tuple, then the return value is
  a nested list or tuple (of the same structure) of <code>2-D</code> tensors with
  the shapes <code>[batch_size, s]</code> for each s in <code>state_size</code>.</p></div>
  <div class="source_cont">
  <p class="source_link"><a href="javascript:void(0);" onclick="toggle('source-latex_ocr.model.components.attention_cell.AttentionCell.zero_state', this);">Show source &equiv;</a></p>
  <div id="source-latex_ocr.model.components.attention_cell.AttentionCell.zero_state" class="source">
    <div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">zero_state</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">,</span> <span class="n">dtype</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Return zero-filled state tensor(s).</span>
<span class="sd">  Args:</span>
<span class="sd">    batch_size: int, float, or unit Tensor representing the batch size.</span>
<span class="sd">    dtype: the data type to use for the state.</span>
<span class="sd">  Returns:</span>
<span class="sd">    If `state_size` is an int or TensorShape, then the return value is a</span>
<span class="sd">    `N-D` tensor of shape `[batch_size, state_size]` filled with zeros.</span>
<span class="sd">    If `state_size` is a nested list or tuple, then the return value is</span>
<span class="sd">    a nested list or tuple (of the same structure) of `2-D` tensors with</span>
<span class="sd">    the shapes `[batch_size, s]` for each s in `state_size`.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="c1"># Try to use the last cached zero_state. This is done to avoid recreating</span>
  <span class="c1"># zeros, especially when eager execution is enabled.</span>
  <span class="n">state_size</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">state_size</span>
  <span class="n">is_eager</span> <span class="o">=</span> <span class="n">context</span><span class="o">.</span><span class="n">executing_eagerly</span><span class="p">()</span>
  <span class="k">if</span> <span class="n">is_eager</span> <span class="ow">and</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s2">&quot;_last_zero_state&quot;</span><span class="p">):</span>
    <span class="p">(</span><span class="n">last_state_size</span><span class="p">,</span> <span class="n">last_batch_size</span><span class="p">,</span> <span class="n">last_dtype</span><span class="p">,</span>
     <span class="n">last_output</span><span class="p">)</span> <span class="o">=</span> <span class="nb">getattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s2">&quot;_last_zero_state&quot;</span><span class="p">)</span>
    <span class="k">if</span> <span class="p">(</span><span class="n">last_batch_size</span> <span class="o">==</span> <span class="n">batch_size</span> <span class="ow">and</span>
        <span class="n">last_dtype</span> <span class="o">==</span> <span class="n">dtype</span> <span class="ow">and</span>
        <span class="n">last_state_size</span> <span class="o">==</span> <span class="n">state_size</span><span class="p">):</span>
      <span class="k">return</span> <span class="n">last_output</span>
  <span class="k">with</span> <span class="n">ops</span><span class="o">.</span><span class="n">name_scope</span><span class="p">(</span><span class="nb">type</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="vm">__name__</span> <span class="o">+</span> <span class="s2">&quot;ZeroState&quot;</span><span class="p">,</span> <span class="n">values</span><span class="o">=</span><span class="p">[</span><span class="n">batch_size</span><span class="p">]):</span>
    <span class="n">output</span> <span class="o">=</span> <span class="n">_zero_state_tensors</span><span class="p">(</span><span class="n">state_size</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">,</span> <span class="n">dtype</span><span class="p">)</span>
  <span class="k">if</span> <span class="n">is_eager</span><span class="p">:</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_last_zero_state</span> <span class="o">=</span> <span class="p">(</span><span class="n">state_size</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">,</span> <span class="n">dtype</span><span class="p">,</span> <span class="n">output</span><span class="p">)</span>
  <span class="k">return</span> <span class="n">output</span>
</pre></div>

  </div>
</div>

  </div>
  
          <h3>Instance variables</h3>
            <div class="item">
            <p id="latex_ocr.model.components.attention_cell.AttentionCell.activity_regularizer" class="name">var <span class="ident">activity_regularizer</span></p>
            

            
  
    <div class="desc"><p>Optional regularizer function for the output of this layer.</p></div>
  <div class="source_cont">
</div>

            </div>
            <div class="item">
            <p id="latex_ocr.model.components.attention_cell.AttentionCell.dtype" class="name">var <span class="ident">dtype</span></p>
            

            
  
  <div class="source_cont">
</div>

            </div>
            <div class="item">
            <p id="latex_ocr.model.components.attention_cell.AttentionCell.graph" class="name">var <span class="ident">graph</span></p>
            

            
  
  <div class="source_cont">
</div>

            </div>
            <div class="item">
            <p id="latex_ocr.model.components.attention_cell.AttentionCell.inbound_nodes" class="name">var <span class="ident">inbound_nodes</span></p>
            

            
  
    <div class="desc"><p>Deprecated, do NOT use! Only for compatibility with external Keras.</p></div>
  <div class="source_cont">
</div>

            </div>
            <div class="item">
            <p id="latex_ocr.model.components.attention_cell.AttentionCell.input" class="name">var <span class="ident">input</span></p>
            

            
  
    <div class="desc"><p>Retrieves the input tensor(s) of a layer.</p>
<p>Only applicable if the layer has exactly one input,
i.e. if it is connected to one incoming layer.</p>
<p>Returns:
    Input tensor or list of input tensors.</p>
<p>Raises:
    AttributeError: if the layer is connected to
    more than one incoming layers.</p>
<p>Raises:
  RuntimeError: If called in Eager mode.
  AttributeError: If no inbound nodes are found.</p></div>
  <div class="source_cont">
</div>

            </div>
            <div class="item">
            <p id="latex_ocr.model.components.attention_cell.AttentionCell.input_mask" class="name">var <span class="ident">input_mask</span></p>
            

            
  
    <div class="desc"><p>Retrieves the input mask tensor(s) of a layer.</p>
<p>Only applicable if the layer has exactly one inbound node,
i.e. if it is connected to one incoming layer.</p>
<p>Returns:
    Input mask tensor (potentially None) or list of input
    mask tensors.</p>
<p>Raises:
    AttributeError: if the layer is connected to
    more than one incoming layers.</p></div>
  <div class="source_cont">
</div>

            </div>
            <div class="item">
            <p id="latex_ocr.model.components.attention_cell.AttentionCell.input_shape" class="name">var <span class="ident">input_shape</span></p>
            

            
  
    <div class="desc"><p>Retrieves the input shape(s) of a layer.</p>
<p>Only applicable if the layer has exactly one input,
i.e. if it is connected to one incoming layer, or if all inputs
have the same shape.</p>
<p>Returns:
    Input shape, as an integer shape tuple
    (or list of shape tuples, one tuple per input tensor).</p>
<p>Raises:
    AttributeError: if the layer has no defined input_shape.
    RuntimeError: if called in Eager mode.</p></div>
  <div class="source_cont">
</div>

            </div>
            <div class="item">
            <p id="latex_ocr.model.components.attention_cell.AttentionCell.losses" class="name">var <span class="ident">losses</span></p>
            

            
  
    <div class="desc"><p>Losses which are associated with this <code>Layer</code>.</p>
<p>Variable regularization tensors are created when this property is accessed,
so it is eager safe: accessing <code>losses</code> under a <code>tf.GradientTape</code> will
propagate gradients back to the corresponding variables.</p>
<p>Returns:
  A list of tensors.</p></div>
  <div class="source_cont">
</div>

            </div>
            <div class="item">
            <p id="latex_ocr.model.components.attention_cell.AttentionCell.name" class="name">var <span class="ident">name</span></p>
            

            
  
  <div class="source_cont">
</div>

            </div>
            <div class="item">
            <p id="latex_ocr.model.components.attention_cell.AttentionCell.non_trainable_variables" class="name">var <span class="ident">non_trainable_variables</span></p>
            

            
  
  <div class="source_cont">
</div>

            </div>
            <div class="item">
            <p id="latex_ocr.model.components.attention_cell.AttentionCell.non_trainable_weights" class="name">var <span class="ident">non_trainable_weights</span></p>
            

            
  
  <div class="source_cont">
</div>

            </div>
            <div class="item">
            <p id="latex_ocr.model.components.attention_cell.AttentionCell.outbound_nodes" class="name">var <span class="ident">outbound_nodes</span></p>
            

            
  
    <div class="desc"><p>Deprecated, do NOT use! Only for compatibility with external Keras.</p></div>
  <div class="source_cont">
</div>

            </div>
            <div class="item">
            <p id="latex_ocr.model.components.attention_cell.AttentionCell.output" class="name">var <span class="ident">output</span></p>
            

            
  
    <div class="desc"><p>Retrieves the output tensor(s) of a layer.</p>
<p>Only applicable if the layer has exactly one output,
i.e. if it is connected to one incoming layer.</p>
<p>Returns:
  Output tensor or list of output tensors.</p>
<p>Raises:
  AttributeError: if the layer is connected to more than one incoming
    layers.
  RuntimeError: if called in Eager mode.</p></div>
  <div class="source_cont">
</div>

            </div>
            <div class="item">
            <p id="latex_ocr.model.components.attention_cell.AttentionCell.output_dtype" class="name">var <span class="ident">output_dtype</span></p>
            

            
  
  <div class="source_cont">
</div>

            </div>
            <div class="item">
            <p id="latex_ocr.model.components.attention_cell.AttentionCell.output_mask" class="name">var <span class="ident">output_mask</span></p>
            

            
  
    <div class="desc"><p>Retrieves the output mask tensor(s) of a layer.</p>
<p>Only applicable if the layer has exactly one inbound node,
i.e. if it is connected to one incoming layer.</p>
<p>Returns:
    Output mask tensor (potentially None) or list of output
    mask tensors.</p>
<p>Raises:
    AttributeError: if the layer is connected to
    more than one incoming layers.</p></div>
  <div class="source_cont">
</div>

            </div>
            <div class="item">
            <p id="latex_ocr.model.components.attention_cell.AttentionCell.output_shape" class="name">var <span class="ident">output_shape</span></p>
            

            
  
    <div class="desc"><p>Retrieves the output shape(s) of a layer.</p>
<p>Only applicable if the layer has one output,
or if all outputs have the same shape.</p>
<p>Returns:
    Output shape, as an integer shape tuple
    (or list of shape tuples, one tuple per output tensor).</p>
<p>Raises:
    AttributeError: if the layer has no defined output shape.
    RuntimeError: if called in Eager mode.</p></div>
  <div class="source_cont">
</div>

            </div>
            <div class="item">
            <p id="latex_ocr.model.components.attention_cell.AttentionCell.output_size" class="name">var <span class="ident">output_size</span></p>
            

            
  
  <div class="source_cont">
</div>

            </div>
            <div class="item">
            <p id="latex_ocr.model.components.attention_cell.AttentionCell.scope_name" class="name">var <span class="ident">scope_name</span></p>
            

            
  
  <div class="source_cont">
</div>

            </div>
            <div class="item">
            <p id="latex_ocr.model.components.attention_cell.AttentionCell.state_size" class="name">var <span class="ident">state_size</span></p>
            

            
  
  <div class="source_cont">
</div>

            </div>
            <div class="item">
            <p id="latex_ocr.model.components.attention_cell.AttentionCell.trainable_variables" class="name">var <span class="ident">trainable_variables</span></p>
            

            
  
  <div class="source_cont">
</div>

            </div>
            <div class="item">
            <p id="latex_ocr.model.components.attention_cell.AttentionCell.trainable_weights" class="name">var <span class="ident">trainable_weights</span></p>
            

            
  
  <div class="source_cont">
</div>

            </div>
            <div class="item">
            <p id="latex_ocr.model.components.attention_cell.AttentionCell.updates" class="name">var <span class="ident">updates</span></p>
            

            
  
  <div class="source_cont">
</div>

            </div>
            <div class="item">
            <p id="latex_ocr.model.components.attention_cell.AttentionCell.variables" class="name">var <span class="ident">variables</span></p>
            

            
  
    <div class="desc"><p>Returns the list of all layer variables/weights.</p>
<p>Alias of <code>self.weights</code>.</p>
<p>Returns:
  A list of variables.</p></div>
  <div class="source_cont">
</div>

            </div>
            <div class="item">
            <p id="latex_ocr.model.components.attention_cell.AttentionCell.weights" class="name">var <span class="ident">weights</span></p>
            

            
  
    <div class="desc"><p>Returns the list of all layer variables/weights.</p>
<p>Returns:
  A list of variables.</p></div>
  <div class="source_cont">
</div>

            </div>
          <h3>Methods</h3>
            
  <div class="item">
    <div class="name def" id="latex_ocr.model.components.attention_cell.AttentionCell.from_config">
    <p>def <span class="ident">from_config</span>(</p><p>cls, config)</p>
    </div>
    

    
  
    <div class="desc"><p>Creates a layer from its config.</p>
<p>This method is the reverse of <code>get_config</code>,
capable of instantiating the same layer from the config
dictionary. It does not handle layer connectivity
(handled by Network), nor weights (handled by <code>set_weights</code>).</p>
<p>Arguments:
    config: A Python dictionary, typically the
        output of get_config.</p>
<p>Returns:
    A layer instance.</p></div>
  <div class="source_cont">
  <p class="source_link"><a href="javascript:void(0);" onclick="toggle('source-latex_ocr.model.components.attention_cell.AttentionCell.from_config', this);">Show source &equiv;</a></p>
  <div id="source-latex_ocr.model.components.attention_cell.AttentionCell.from_config" class="source">
    <div class="codehilite"><pre><span></span><span class="nd">@classmethod</span>
<span class="k">def</span> <span class="nf">from_config</span><span class="p">(</span><span class="bp">cls</span><span class="p">,</span> <span class="n">config</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Creates a layer from its config.</span>
<span class="sd">  This method is the reverse of `get_config`,</span>
<span class="sd">  capable of instantiating the same layer from the config</span>
<span class="sd">  dictionary. It does not handle layer connectivity</span>
<span class="sd">  (handled by Network), nor weights (handled by `set_weights`).</span>
<span class="sd">  Arguments:</span>
<span class="sd">      config: A Python dictionary, typically the</span>
<span class="sd">          output of get_config.</span>
<span class="sd">  Returns:</span>
<span class="sd">      A layer instance.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="k">return</span> <span class="bp">cls</span><span class="p">(</span><span class="o">**</span><span class="n">config</span><span class="p">)</span>
</pre></div>

  </div>
</div>

  </div>
  
      </div>
      </div>
      
      <div class="item">
      <p id="latex_ocr.model.components.attention_cell.AttentionState" class="name">class <span class="ident">AttentionState</span></p>
      
  
    <div class="desc"><p>AttentionState(cell_state, o)</p></div>
  <div class="source_cont">
</div>


      <div class="class">
          <h3>Ancestors (in MRO)</h3>
          <ul class="class_list">
          <li><a href="#latex_ocr.model.components.attention_cell.AttentionState">AttentionState</a></li>
          <li>builtins.tuple</li>
          <li>builtins.object</li>
          </ul>
          <h3>Instance variables</h3>
            <div class="item">
            <p id="latex_ocr.model.components.attention_cell.AttentionState.cell_state" class="name">var <span class="ident">cell_state</span></p>
            

            
  
    <div class="desc"><p>Alias for field number 0</p></div>
  <div class="source_cont">
</div>

            </div>
            <div class="item">
            <p id="latex_ocr.model.components.attention_cell.AttentionState.o" class="name">var <span class="ident">o</span></p>
            

            
  
    <div class="desc"><p>Alias for field number 1</p></div>
  <div class="source_cont">
</div>

            </div>
      </div>
      </div>

  </section>

    </article>
  <div class="clear"> </div>
  <footer id="footer">
    <p>
      Documentation generated by
      <a href="https://github.com/BurntSushi/pdoc">pdoc 0.3.2</a>
    </p>

    <p>pdoc is in the public domain with the
      <a href="http://unlicense.org">UNLICENSE</a></p>

    <p>Design by <a href="http://nadh.in">Kailash Nadh</a></p>
  </footer>
</div>
</body>
</html>
